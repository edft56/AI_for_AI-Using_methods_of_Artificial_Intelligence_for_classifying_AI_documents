{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6678,"status":"ok","timestamp":1639382885657,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"-ROYuT_tyV46","outputId":"64b85423-0bca-456f-94b9-534b499fb23d"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import tensorflow as tf\n","import tensorflow_text as tf_text\n","import tensorflow_hub as tf_hub\n","from tensorflow.keras import mixed_precision\n","from official.nlp import optimization  # to create AdamW optimizer\n","\n","device_to_use = 'CPU' #SET GLOBAL DEVICE\n","#device_to_use = 'GPU' #SET GLOBAL DEVICE\n","\n","USE_SOFTMAX = False\n","TRAIN_MODEL = False\n","\n","\n","if (device_to_use=='GPU'):\n","    #mixed_precision.set_global_policy('mixed_float16') #tf bug, cant save model when using fp16 :(\n","    #tf.config.optimizer.set_jit(True) # Enable XLA. SOMETIMES IT DOESNT WORK AND U CANT FIGURE OUT WHATS WRONG\n","    pass\n","else:\n","    # Hide GPU from visible devices\n","    tf.config.set_visible_devices([], 'GPU')\n","\n","print(tf.config.get_visible_devices())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def read_json_data(filename):\n","  with open(filename, 'r') as json_file:\n","    dataset_dict = json.load(json_file)\n","\n","  dataset = []\n","\n","  for _,group in dataset_dict.items():\n","    group_data = []\n","\n","    for _,data in group.items():\n","      group_data.append(data)\n","\n","    dataset.append(group_data)\n","\n","  train_set = dataset[0]\n","  val_set = dataset[1]\n","  test_set = dataset[2]\n","\n","  return train_set,val_set,test_set\n","\n","def remove_copyright(text):\n","\n","  copyright_idx = text.find('Â©')\n","  if(copyright_idx!=-1):\n","    text = text[:copyright_idx]\n","\n","  return text\n","\n","\n","def create_tf_dataset(text,labels,preprocess_model=None,softmax=False,shuffle=True):\n","  #if categorical cross entropy loss is used, one-hot encode labels\n","  if(softmax and (labels is not None) ):\n","    labels = tf.one_hot(labels,2)\n","\n","  if(labels is not None):\n","    dataset = tf.data.Dataset.from_tensor_slices( (text,labels) )\n","  else:\n","    dataset = tf.data.Dataset.from_tensor_slices( text )\n","\n","  if(shuffle): dataset = dataset.shuffle(32768)\n","  \n","  dataset = dataset.batch(8)\n","\n","  #apply pre-process model to each batch\n","  if (preprocess_model is not None and (labels is not None)):\n","    dataset = dataset.map(lambda x,y: (preprocess_model(x), y))\n","  elif (preprocess_model is not None and (labels is None)):\n","    dataset = dataset.map(lambda x: preprocess_model(x))\n","\n","  #prefetch batches so that the gpu doesnt starve\n","  dataset = dataset.prefetch(8)\n","\n","  return dataset\n","\n","def get_datasets_and_metadata(filename):\n","\n","  #get data\n","  train_set,val_set,test_set = read_json_data(filename)\n","\n","  #iterate over the train validation and test set and remove all copyright messages\n","  train_abs = [None] * len(train_set[0])\n","  for idx,abs in enumerate(train_set[0]):\n","    train_abs[idx] = remove_copyright(abs)\n","\n","  val_abs = [None] * len(val_set[0])\n","  for idx,abs in enumerate(val_set[0]):\n","    val_abs[idx] = remove_copyright(abs)\n","\n","  test_abs = [None] * len(test_set[0])\n","  for idx,abs in enumerate(test_set[0]):\n","    test_abs[idx] = remove_copyright(abs)\n","\n","  train_set[0] = train_abs\n","  val_set[0] = val_abs\n","  test_set[0] = test_abs\n","\n","  return train_set,val_set,test_set"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4612,"status":"ok","timestamp":1639383964956,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"KFDC0sqobb82","outputId":"30ea21f7-ebc7-48a4-a5d1-dce898c1a502"},"outputs":[],"source":["#select model to be used\n","bert_model_name = 'small_bert/bert_en_uncased_L-6_H-768_A-12'\n","\n","map_name_to_handle = {\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1'\n","}\n","\n","map_model_to_preprocess = {\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n","\n","#get model from tf hub\n","bert_preprocess_model = tf_hub.KerasLayer(tfhub_handle_preprocess)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#define models\n","\n","#abstracts are usually between 250-500 words so a sequence length of 512 is desirable\n","def build_preprocess_model(sentence_features, seq_length=512):\n","  input_segments = [\n","      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n","      for ft in sentence_features]\n","\n","  # Tokenize the text to word pieces.\n","  bert_preprocess = tf_hub.load(tfhub_handle_preprocess)\n","  tokenizer = tf_hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n","  segments = [tokenizer(s) for s in input_segments]\n","\n","  # Pack inputs. The details (start/end token ids, dict of output tensors)\n","  # are model-dependent, so this gets loaded from the SavedModel.\n","  packer = tf_hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n","                          arguments=dict(seq_length=seq_length),\n","                          name='packer')\n","  model_inputs = packer(segments)\n","  return tf.keras.Model(input_segments, model_inputs)\n","\n","def build_classifier_model(num_classes=2, softmax=False):\n","    \n","    class Classifier(tf.keras.Model):\n","        def __init__(self):\n","            super(Classifier, self).__init__(name=\"prediction\")\n","            self.encoder = tf_hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","\n","            if(softmax): \n","               self.dense = tf.keras.layers.Dense(num_classes,activation='softmax')\n","            else:\n","                self.dense = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')\n","\n","        def call(self, preprocessed_text):\n","            encoder_outputs = self.encoder(preprocessed_text)\n","            pooled_output = encoder_outputs[\"pooled_output\"]\n","            x = self.dense(pooled_output)\n","            return x\n","\n","    model = Classifier()\n","    return model\n","\n","\n","def get_model_predictions(dataset,model,softmax=False):\n","    predictions = model.predict(dataset)\n","    \n","    if (softmax):\n","        yes_scores = predictions[:,1]\n","        no_scores = predictions[:,0]\n","        \n","        positive = np.count_nonzero(yes_scores>no_scores)\n","        negative = np.count_nonzero(yes_scores<no_scores)\n","    else:\n","        positive = np.count_nonzero(predictions>=0.5)\n","        negative = np.count_nonzero(predictions<0.5)\n","\n","    print(float(positive)/(positive+negative))\n","\n","    return predictions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#get data and create train and validation tf datasets\n","\n","train_set,val_set,test_set = get_datasets_and_metadata(\"AI_Paper_Classification_Dataset.json\")\n","\n","preprocess_model = build_preprocess_model([\"input 1\"]) \n","\n","train_dataset = create_tf_dataset(train_set[0],train_set[1],preprocess_model,softmax=USE_SOFTMAX)\n","val_dataset = create_tf_dataset(val_set[0],val_set[1],preprocess_model,softmax=USE_SOFTMAX)\n","\n","classifier_model = build_classifier_model(softmax=USE_SOFTMAX)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4324590,"status":"ok","timestamp":1639388289532,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"LyaZ9DE-M7bd","outputId":"111b1609-bb3b-4e85-9704-2098a9d637f8"},"outputs":[],"source":["# train the model or load previous weights\n","\n","if (USE_SOFTMAX):\n","    loss = tf.keras.losses.CategoricalCrossentropy()\n","else:\n","    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","\n","#select required metrics \n","metrics = [ tf.keras.metrics.CategoricalAccuracy() if USE_SOFTMAX else tf.keras.metrics.BinaryAccuracy() ,\n","            tf.keras.metrics.AUC(),\n","            tf.keras.metrics.FalsePositives(),\n","            tf.keras.metrics.FalseNegatives(),\n","            tf.keras.metrics.TruePositives(),\n","            tf.keras.metrics.TrueNegatives(),\n","            tf.keras.metrics.Precision(),\n","            tf.keras.metrics.Recall()\n","        ]\n","\n","\n","epochs = 1\n","steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","\n","optimizer = optimization.create_optimizer(init_lr=3e-5,\n","                                        num_train_steps=num_train_steps,\n","                                        end_lr=1e-6,\n","                                        num_warmup_steps=num_warmup_steps,\n","                                        optimizer_type='adamw')\n","\n","\n","classifier_model.compile(optimizer=optimizer,\n","                        loss=loss,\n","                        metrics=metrics)\n","\n","\n","if (TRAIN_MODEL):\n","    history = classifier_model.fit(x=train_dataset,\n","                                validation_data=val_dataset,\n","                                epochs=epochs\n","                                )\n","\n","    classifier_model.save_weights('./bert_weights/')\n","    \n","else:\n","    classifier_model.load_weights('./bert_weights/')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# run model on the validation set\n","classifier_model.evaluate(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#get prediction on the test set\n","test_dataset = create_tf_dataset(test_set[0],None,preprocess_model,shuffle=False,softmax=USE_SOFTMAX)\n","predictions = get_model_predictions(test_dataset,classifier_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Using this function we can extract statistics from the predictions\n","#We can find out percentage of papers predicted as AI per subject,per year per journal etc.\n","#the data need to be argsorted and then using that index we need to sort the data and the preictions\n","#if we have a list of X abstracts, X subjects, X predictions, argsort the subjects\n","#using that index sort predictions and subjects\n","#input the sorted subjects and predictions to this function\n","#percentages of AI predictions per subject are printed\n","def extract_data(data_sorted,predictions_sorted,softmax=False):\n","  # percent AI predicted per unique value\n","  percent_AI_per_value = []\n","  # unique values\n","  values = [data_sorted[0]]\n","\n","  total_count_per_value = 0\n","  pos_count_per_value = 0\n","\n","  for idx,prediction in enumerate(predictions_sorted):\n","    if data_sorted[idx] != values[-1]:\n","      values.append(data_sorted[idx])\n","      percent_AI_per_value.append(float(pos_count_per_value)/total_count_per_value)\n","      total_count_per_value = 0\n","      pos_count_per_value = 0\n","    \n","    total_count_per_value += 1\n","    if(softmax):\n","      if prediction[1]>prediction[0]:\n","        pos_count_per_value += 1\n","    else:\n","      if prediction>=0.5:\n","        pos_count_per_value += 1\n","\n","  percent_AI_per_value.append(float(pos_count_per_value)/total_count_per_value)\n","\n","  for i in range(len(values)):\n","    print(values[i]) \n","    print(percent_AI_per_value[i])\n","\n","\n","\n","subjects = np.array(test_set[1])\n","subject_sort_idx = np.argsort(subjects.astype(np.str))\n","subj_sorted = subjects[subject_sort_idx]\n","predictions_subj_sort = predictions[subject_sort_idx]\n","extract_data(subj_sorted,predictions_subj_sort)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10644,"status":"ok","timestamp":1639044609870,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"95-WpMj-FUlR","outputId":"ab6fa569-d4b0-40eb-81ae-ba71135d88d7"},"outputs":[],"source":["#print abstracts of a certain category predicted as AI\n","\n","cnt = 0\n","abstracts_to_show = 10\n","category = 'Computer_Science'\n","\n","for idx,prediction in enumerate(predictions_subj_sort):\n","    if subj_sorted[idx]==category and prediction>=0.5:\n","        if(cnt<abstracts_to_show):\n","            print(test_set[0][subject_sort_idx[idx]])\n","            print(\"\")\n","        cnt+=1\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#create excel with abstracts predicted as AI per subject area\n","\n","current_subj = subj_sorted[0]\n","abstracts_list = []\n","writer = pd.ExcelWriter('AI_Predictions_per_Subject_Area.xlsx', engine = 'openpyxl')\n","\n","for idx,prediction in enumerate(predictions_subj_sort):\n","    if subj_sorted[idx] != current_subj:\n","        df = pd.DataFrame(abstracts_list)\n","        df.to_excel(writer, sheet_name = current_subj)\n","        writer.save()\n","        abstracts_list = []\n","        current_subj = subj_sorted[idx]\n","    \n","    if(prediction>=0.5):\n","        abstracts_list.append(test_set[0][subject_sort_idx[idx]])\n","\n","writer.close()"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPluOjuDu2gYWa0ozygev/9","collapsed_sections":[],"name":"AI_Classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
