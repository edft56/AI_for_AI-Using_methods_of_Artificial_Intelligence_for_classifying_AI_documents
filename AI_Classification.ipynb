{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6678,"status":"ok","timestamp":1639382885657,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"-ROYuT_tyV46","outputId":"64b85423-0bca-456f-94b9-534b499fb23d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]},{"name":"stderr","output_type":"stream","text":["2022-02-03 10:18:53.842890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-02-03 10:18:53.875166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-02-03 10:18:53.875360: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow_text as tf_text\n","import tensorflow_hub as tf_hub\n","from tensorflow.keras import mixed_precision\n","from official.nlp import optimization  # to create AdamW optimizer\n","\n","#device_to_use = 'CPU' #SET GLOBAL DEVICE\n","device_to_use = 'GPU' #SET GLOBAL DEVICE\n","\n","USE_SOFTMAX = False\n","TRAIN_MODEL = False\n","\n","\n","if (device_to_use=='GPU'):\n","    #mixed_precision.set_global_policy('mixed_float16') #tf bug, cant save model when using fp16 :(\n","    #tf.config.optimizer.set_jit(True) # Enable XLA. SOMETIMES IT DOESNT WORK AND U CANT FIGURE OUT WHATS WRONG\n","    pass\n","else:\n","    # Hide GPU from visible devices\n","    tf.config.set_visible_devices([], 'GPU')\n","\n","print(tf.config.get_visible_devices())"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def read_json_data(filename):\n","  import json\n","  with open(filename, 'r') as json_file:\n","    dataset_dict = json.load(json_file)\n","\n","  dataset = []\n","\n","  for _,group in dataset_dict.items():\n","    group_data = []\n","\n","    for _,data in group.items():\n","      group_data.append(data)\n","\n","    dataset.append(group_data)\n","\n","  train_set = dataset[0]\n","  val_set = dataset[1]\n","  test_set = dataset[2]\n","\n","  return train_set,val_set,test_set\n","\n","def remove_copyright(text):\n","\n","  copyright_idx = text.find('©')\n","  if(copyright_idx!=-1):\n","    text = text[:copyright_idx]\n","\n","  return text\n","\n","\n","def create_tf_dataset(text,labels,preprocess_model=None,softmax=False,shuffle=True):\n","  if(softmax and (labels is not None) ):\n","    labels = tf.one_hot(labels,2)\n","\n","  if(labels is not None):\n","    dataset = tf.data.Dataset.from_tensor_slices( (text,labels) )\n","  else:\n","    dataset = tf.data.Dataset.from_tensor_slices( text )\n","\n","  if(shuffle): dataset = dataset.shuffle(32768)\n","  \n","  dataset = dataset.batch(8)\n","\n","  if (preprocess_model is not None and (labels is not None)):\n","    dataset = dataset.map(lambda x,y: (preprocess_model(x), y))\n","  elif (preprocess_model is not None and (labels is None)):\n","    dataset = dataset.map(lambda x: preprocess_model(x))\n","\n","  dataset = dataset.prefetch(8)\n","\n","  return dataset\n","\n","def get_datasets_and_metadata(filename):\n","  train_set,val_set,test_set = read_json_data(filename)\n","\n","  train_abs = [None] * len(train_set[0])\n","  for idx,abs in enumerate(train_set[0]):\n","    train_abs[idx] = remove_copyright(abs)\n","\n","  val_abs = [None] * len(val_set[0])\n","  for idx,abs in enumerate(val_set[0]):\n","    val_abs[idx] = remove_copyright(abs)\n","\n","  test_abs = [None] * len(test_set[0])\n","  for idx,abs in enumerate(test_set[0]):\n","    test_abs[idx] = remove_copyright(abs)\n","\n","  train_set[0] = train_abs\n","  val_set[0] = val_abs\n","  test_set[0] = test_abs\n","\n","  return train_set,val_set,test_set"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4612,"status":"ok","timestamp":1639383964956,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"KFDC0sqobb82","outputId":"30ea21f7-ebc7-48a4-a5d1-dce898c1a502"},"outputs":[{"name":"stdout","output_type":"stream","text":["BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1\n","Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"]},{"name":"stderr","output_type":"stream","text":["2022-02-03 10:19:07.016493: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2022-02-03 10:19:07.017331: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-02-03 10:19:07.017598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-02-03 10:19:07.017773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-02-03 10:19:07.336191: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-02-03 10:19:07.336356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-02-03 10:19:07.336487: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","2022-02-03 10:19:07.336612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7618 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:2b:00.0, compute capability: 8.6\n"]}],"source":["bert_model_name = 'small_bert/bert_en_uncased_L-6_H-768_A-12'\n","\n","map_name_to_handle = {\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1'\n","}\n","\n","map_model_to_preprocess = {\n","    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n","        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n","}\n","\n","tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n","tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n","\n","print(f'BERT model selected           : {tfhub_handle_encoder}')\n","print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')\n","\n","bert_preprocess_model = tf_hub.KerasLayer(tfhub_handle_preprocess)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def build_preprocess_model(sentence_features, seq_length=512):\n","  input_segments = [\n","      tf.keras.layers.Input(shape=(), dtype=tf.string, name=ft)\n","      for ft in sentence_features]\n","\n","  # Tokenize the text to word pieces.\n","  bert_preprocess = tf_hub.load(tfhub_handle_preprocess)\n","  tokenizer = tf_hub.KerasLayer(bert_preprocess.tokenize, name='tokenizer')\n","  segments = [tokenizer(s) for s in input_segments]\n","\n","  # Pack inputs. The details (start/end token ids, dict of output tensors)\n","  # are model-dependent, so this gets loaded from the SavedModel.\n","  packer = tf_hub.KerasLayer(bert_preprocess.bert_pack_inputs,\n","                          arguments=dict(seq_length=seq_length),\n","                          name='packer')\n","  model_inputs = packer(segments)\n","  return tf.keras.Model(input_segments, model_inputs)\n","\n","def build_classifier_model(num_classes=2, softmax=False):\n","    \n","    class Classifier(tf.keras.Model):\n","        def __init__(self):\n","            super(Classifier, self).__init__(name=\"prediction\")\n","            self.encoder = tf_hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n","\n","            if(softmax): \n","               self.dense = tf.keras.layers.Dense(num_classes,activation='softmax')\n","            else:\n","                self.dense = tf.keras.layers.Dense(1, activation='sigmoid', name='classifier')\n","\n","        def call(self, preprocessed_text):\n","            encoder_outputs = self.encoder(preprocessed_text)\n","            pooled_output = encoder_outputs[\"pooled_output\"]\n","            x = self.dense(pooled_output)\n","            return x\n","\n","    model = Classifier()\n","    return model\n","\n","def get_model_predictions(dataset,model,softmax=False):\n","    predictions = model.predict(dataset)\n","    \n","    if (softmax):\n","        yes_scores = predictions[:,1]\n","        no_scores = predictions[:,0]\n","        \n","        positive = np.count_nonzero(yes_scores>no_scores)\n","        negative = np.count_nonzero(yes_scores<no_scores)\n","    else:\n","        positive = np.count_nonzero(predictions>=0.5)\n","        negative = np.count_nonzero(predictions<0.5)\n","\n","    print(float(positive)/(positive+negative))\n","\n","    return predictions"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["train_set,val_set,test_set = get_datasets_and_metadata(\"AI_Paper_Classification_Dataset2.json\")\n","\n","preprocess_model = build_preprocess_model([\"input 1\"])\n","\n","train_dataset = create_tf_dataset(train_set[0],train_set[1],preprocess_model,softmax=USE_SOFTMAX)\n","val_dataset = create_tf_dataset(val_set[0],val_set[1],preprocess_model,softmax=USE_SOFTMAX)\n","\n","classifier_model = build_classifier_model(softmax=USE_SOFTMAX)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4324590,"status":"ok","timestamp":1639388289532,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"LyaZ9DE-M7bd","outputId":"111b1609-bb3b-4e85-9704-2098a9d637f8"},"outputs":[],"source":["if (USE_SOFTMAX):\n","    loss = tf.keras.losses.CategoricalCrossentropy()\n","else:\n","    loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","\n","\n","metrics = [ tf.keras.metrics.CategoricalAccuracy() if USE_SOFTMAX else tf.keras.metrics.BinaryAccuracy() ,\n","            tf.keras.metrics.AUC(),\n","            tf.keras.metrics.FalsePositives(),\n","            tf.keras.metrics.FalseNegatives(),\n","            tf.keras.metrics.TruePositives(),\n","            tf.keras.metrics.TrueNegatives(),\n","            tf.keras.metrics.Precision(),\n","            tf.keras.metrics.Recall()\n","        ]\n","\n","\n","epochs = 1\n","steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n","num_train_steps = steps_per_epoch * epochs\n","num_warmup_steps = int(0.1*num_train_steps)\n","\n","\n","optimizer = optimization.create_optimizer(init_lr=3e-5,\n","                                        num_train_steps=num_train_steps,\n","                                        end_lr=1e-6,\n","                                        num_warmup_steps=num_warmup_steps,\n","                                        optimizer_type='adamw')\n","\n","\n","classifier_model.compile(optimizer=optimizer,\n","                        loss=loss,\n","                        metrics=metrics)\n","\n","\n","if (TRAIN_MODEL):\n","    history = classifier_model.fit(x=train_dataset,\n","                                validation_data=val_dataset,\n","                                epochs=epochs\n","                                )\n","\n","    classifier_model.save_weights('./bert_weights3/')\n","    \n","else:\n","    classifier_model.load_weights('./bert_weights3/')"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["13189/13189 [==============================] - 628s 48ms/step - loss: 0.1338 - binary_accuracy: 0.9645 - auc: 0.9908 - false_positives: 2581.0000 - false_negatives: 1165.0000 - true_positives: 41959.0000 - true_negatives: 59807.0000 - precision: 0.9421 - recall: 0.9730\n"]},{"data":{"text/plain":["[0.13383325934410095,\n"," 0.964496910572052,\n"," 0.9908130168914795,\n"," 2581.0,\n"," 1165.0,\n"," 41959.0,\n"," 59807.0,\n"," 0.9420520663261414,\n"," 0.9729849100112915]"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["classifier_model.evaluate(val_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_dataset = create_tf_dataset(test_set[0],None,preprocess_model,shuffle=False,softmax=USE_SOFTMAX)\n","predictions = get_model_predictions(test_dataset,classifier_model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1222186,"status":"ok","timestamp":1639391412818,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"SCrTLJeAOi1f","outputId":"cba1f253-939f-40ae-8de7-4b3aea337791"},"outputs":[{"name":"stdout","output_type":"stream","text":["Statistical Methodology.xlsx\n"]},{"name":"stderr","output_type":"stream","text":["2022-02-01 11:13:38.614584: I tensorflow/compiler/xla/service/service.cc:171] XLA service 0x7f16b400bf00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n","2022-02-01 11:13:38.614623: I tensorflow/compiler/xla/service/service.cc:179]   StreamExecutor device (0): NVIDIA GeForce RTX 3080, Compute Capability 8.6\n","2022-02-01 11:13:38.671466: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n","2022-02-01 11:13:43.070009: I tensorflow/compiler/jit/xla_compilation_cache.cc:351] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","2022-02-01 11:13:46.873318: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"]},{"name":"stdout","output_type":"stream","text":["0.3375565610859729\n","Agricultural Systems.xlsx\n","0.01001001001001001\n","Robotics.xlsx\n","0.6155\n","Signal Processing.xlsx\n","0.37104072398190047\n","Molecular Biology.xlsx\n","0.011182108626198083\n","Economics.xlsx\n","0.023099850968703428\n","Medicine.xlsx\n","0.0042643923240938165\n","Neurology.xlsx\n","0.023578363384188627\n","Network.xlsx\n","0.24266365688487584\n","Archaeology.xlsx\n","0.009013520280420632\n","Political Psychology.xlsx\n","0.010771992818671455\n"]}],"source":["test_files = {\n","            'Statistical Methodology.xlsx':'Statistical Methodology.xlsx',\n","            'Agricultural Systems.xlsx':'Agricultural Systems.xlsx',\n","            'Robotics.xlsx':'Robotics.xlsx',\n","            'Signal Processing.xlsx':'Signal Processing.xlsx',\n","            'Molecular Biology.xlsx':'Molecular Biology.xlsx',\n","            'Economics.xlsx':'Economics.xlsx',\n","            'Medicine.xlsx':'Medicine.xlsx',\n","            'Neurology.xlsx':'Neurology.xlsx',\n","            'Network.xlsx':'Network.xlsx',\n","            'Archaeology.xlsx':'Archaeology.xlsx',\n","            'Political Psychology.xlsx':'Political Psychology.xlsx'\n","            }\n","\n","for key,value in test_files.items():\n","  #if(key=='Archaeology.xlsx'):\n","    test_abs = pd.read_excel(value,sheet_name = 'Sheet1')\n","    test_abs = test_abs.loc[:,['Abstract']]\n","    test_abs = test_abs.to_numpy().squeeze()\n","\n","    test_abs2 = [None] * len(test_abs)\n","    for idx,abs in enumerate(test_abs):\n","      test_abs2[idx] = remove_copyright(abs)\n","\n","\n","    test_abs2 = create_tf_dataset(test_abs2,None,preprocess_model,softmax=USE_SOFTMAX,shuffle=False)\n","\n","    print(key)\n","    get_model_predictions(test_abs2,classifier_model,softmax=USE_SOFTMAX)\n","    \n","    # count = 0\n","    # max_shown = 5\n","    # for i in range(len(yes_scores)):\n","    #   if yes_scores[i]>no_scores[i]:\n","    #     print(yes_scores[i])\n","    #     print(abs_text[i])\n","    #     count= count + 1\n","    #   if (max_shown == count):\n","    #     break\n","\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-02-03 10:19:42.403051: I tensorflow/stream_executor/cuda/cuda_blas.cc:1774] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"]},{"name":"stdout","output_type":"stream","text":["0.03999166506379432\n"]}],"source":[]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Agricultural_and_Biological_Sciences\n","0.00979706088173548\n","Arts_and_humanities\n","0.034565409360413774\n","Biochemistry, Genetics and Molecular Biology\n","0.014830508474576272\n","Business_management_accounting\n","0.03364692861095739\n","Chemical_Engineering\n","0.01562027612617152\n","Chemistry\n","0.024121709538584727\n","Computer_Science\n","0.24453704644780974\n","Earth_and_Planetary_sciences\n","0.028452814904819764\n","Engineering\n","0.0671808402679115\n","Environmental_Sciences\n","0.016875505254648343\n","Health_professions\n","0.049488583531356545\n","Immunology_and_Microbiology\n","0.006179196704428424\n","Material_Science\n","0.010258473297797445\n","Mathematics\n","0.11201381131309028\n","Medicine\n","0.008932324506094998\n","Neuroscience\n","0.03946297803091944\n","Nursing\n","0.004123022063739693\n","Pharmacology_Toxicology,Pharmaceutics\n","0.012663357309289839\n","Physics_and_Astronomy\n","0.02299082383785419\n","Psychology\n","0.01858736059479554\n","Social_Sciences\n","0.03219978746014878\n","decision_science\n","0.17195956295312978\n","dentistry\n","0.00314828418511911\n","economics_econometrics\n","0.017331407621375403\n","energy\n","0.029370348018507342\n","veterinary\n","0.001336898395721925\n"]}],"source":["def extract_data(data_sorted,predictions_sorted,softmax=False):\n","  percent_AI_per_value = []\n","  values = [data_sorted[0]]\n","\n","  total_count_per_value = 0\n","  pos_count_per_value = 0\n","\n","  for idx,prediction in enumerate(predictions_sorted):\n","    if data_sorted[idx] != values[-1]:\n","      values.append(data_sorted[idx])\n","      percent_AI_per_value.append(float(pos_count_per_value)/total_count_per_value)\n","      total_count_per_value = 0\n","      pos_count_per_value = 0\n","    \n","    total_count_per_value += 1\n","    if(softmax):\n","      if prediction[1]>prediction[0]:\n","        pos_count_per_value += 1\n","    else:\n","      if prediction>=0.5:\n","        pos_count_per_value += 1\n","\n","  percent_AI_per_value.append(float(pos_count_per_value)/total_count_per_value)\n","\n","  for i in range(len(values)):\n","    print(values[i]) \n","    print(percent_AI_per_value[i])\n","\n","\n","\n","subjects = np.array(test_set[1])\n","subject_sort_idx = np.argsort(subjects.astype(np.str))\n","subj_sorted = subjects[subject_sort_idx]\n","predictions_subj_sort = predictions[subject_sort_idx]\n","extract_data(subj_sorted,predictions_subj_sort)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10644,"status":"ok","timestamp":1639044609870,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"95-WpMj-FUlR","outputId":"ab6fa569-d4b0-40eb-81ae-ba71135d88d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["This paper presents a novel optimization-based approach to compute time-optimal trajectories for robotic systems operating in an environment with the presence of obstacles under kinodynamic constraints. The proposed approach employs a modified rapid exploring random tree algorithm (RRT) to generate a geometrical sub-optimal path inside a feasible safe region. Subsequently, a trajectory is parametrized by fourth order non-uniform B-splines and is optimized along the path with respect to kinodynamic constraints by an interior point optimizer. The optimization process is performed in the safe region without any further collision checking, which is very effective in extremely confined and complex environments. Finally, the potential and efficiency of the approach is illustrated and compared with the notable RRT* algorithm in state space by numerical simulations. \n","\n","Protein-Protein Interactions (PPIs) play a vital role in most cellular processes. Although many efforts have been devoted to detecting protein interactions by high-throughput experiments, these methods are obviously expensive and tedious. Targeting these inevitable disadvantages, this study develops a novel computational method to predict PPIs using information on protein sequences, which is highly efficient and accurate. The improvement mainly comes from the use of the Rotation Forest (RF) classifier and the Local Phase Quantization (LPQ) descriptor from the Physicochemical Property Response (PR) Matrix of protein amino acids. When performed on three PPI datasets including Saccharomyces cerevisiae, Homo sapiens, and Helicobacter pylori, we obtained good results of average accuracies of 93.8%, 97.96%, and 89.47%, which are much better than in previous studies. Extensive validations have also been explored to evaluate the performance of the Rotation Forest ensemble classifier with the state-of-the-art Support Vector Machine classifier. These promising results indicate that the proposed method might play a complementary role for future proteomics research. \n","\n","Data augmentation has recently become an important method for improving performance in deep learning. It is also a significant issue in machine translation, and various innovations such as back-translation and noising have been made. In particular, current state-of-the-art model architectures such as BERT-fused or efficient data generation using the GPT model provide good inspiration to improve the translation performance. In this study, we propose the generation of additional data for neural machine translation (NMT) using a sentence generator by GPT-2 that produces similar characteristics to the original. BERT-fused architecture and back-translation are employed for the translation architecture. In our experiments, the model produced BLEU scores of 27.50 for tatoebaEn-Ja, 30.14 for WMT14En-De, and 24.12 for WMT18En-Ch. \n","\n","A cable-stayed bridge is a typical symmetrical structure, and symmetry affects the deformation characteristics of such bridges. The main girder of a cable-stayed bridge will produce obvious deflection under the inducement of temperature. The regression model of temperature-induced deflection is hoped to provide a comparison value for bridge evaluation. Based on the temperature and deflection data obtained by the health monitoring system of a bridge, establishing the correlation model between temperature and temperature-induced deflection is meaningful. It is difficult to complete a high-quality model only by the girder temperature. The temperature features based on prior knowledge from the mechanical mechanism are used as the input information in this paper. At the same time, to strengthen the nonlinear ability of the model, this paper selects an independent recurrent neural network (IndRNN) for modeling. The deep learning neural network is compared with machine learning neural networks to prove the advancement of deep learning. When only the average temperature of the main girder is input, the calculation accuracy is not high regardless of whether the deep learning network or the machine learning network is used. When the temperature information extracted by the prior knowledge is input, the average error of IndRNN model is only 2.53%, less than those of BPNN model and traditional RNN. Combining knowledge with deep learning is undoubtedly the best modeling scheme. The deep learning model can provide a comparison value of bridge deformation for bridge management. \n","\n","Several algorithms have been developed for problems of data aggregation in wireless sensor networks, all of which tried to increase networks lifetime. In this article, we deal with this problem using a more efficient method, and offer an heuristic algorithm based on distributed learning automata to solve data aggregation problems within stochastic graphs. Given that data aggregating through creating backbones and making connected dominating sets (CDS) in networks lowers the ratio of responding hosts to the hosts existing in virtual backbones, we employed this idea to our algorithm, trying to increase networks lifetime considering such parameters as sensors lifetime, remaining and consumption energies in order to have an almost optimal data aggregation within networks. Finally, we evaluate our algorithm for made CDS lifetime given increased sensor radius and increased sensors number.\n","\n","The key objective of the proposed work in this paper is to introduce a new version of picture linguistic fuzzy set, so-called spherical linguistic fuzzy sets. The novel concept of spherical linguistic fuzzy set consists of linguistic term, positive, neutral and negative membership degrees which satisfies the conditions that the square sum of its membership degrees is less than or equal to 1. In this paper, we investigate the basic operations of spherical linguistic fuzzy sets and discuss some related results. We extend operational laws of aggregation operators and propose spherical linguistic fuzzy Choquet integral weighted averaging (SLFCIWA) operator based on spherical fuzzy numbers. Further, the proposed SLFCIWA operator of spherical fuzzy number is applied to multi-attribute group decision-making problems. Also, we propose the GRA method to aggregate the spherical fuzzy information. To implement the proposed models, we provide some numerical applications of group decision-making problems. Also compared with the previous model, we conclude that the proposed technique is more effective and reliable. \n","\n","Road accident data analysis plays an important role in identifying key factors associated with road accidents. These associated factors help in taking preventive measures to overcome the road accidents. Various studies have been done on road accident data analysis using traditional statistical techniques and data mining techniques. All these studies focused on identifying key factors associated with road accidents in different countries. Road accident is uncertain and unpredictable events which can occur in any circumstances. Also, road accidents do not have similar impacts in every region of the districts. There are chances that road accident rate is increasing in a certain district but it has some lower impact in other districts. Hence, the more focus on road safety should be on those regions or districts where road accident trend is increasing. Time series analysis is an important area of study which can be helpful in identifying the increasing or decreasing trends in different districts. In this paper, we have proposed a framework to analyze road accident time series data that takes 39 time series data of 39 districts of Gujrat and Uttarakhand state of India. This framework segments the time series data into different clusters. A time series merging algorithm is proposed to find the representative time series (RTS) for each cluster. This RTS is further used for trend analysis of different clusters. The results reveals that road accident trend is going to increase in certain clusters and those districts should be the prime concern to take preventive measure to overcome the road accidents. \n","\n","For video sequences, the visibility of distortion depends upon the motion present in the video, as the temporal activity can mask the distortion. For videos with high temporal activity, the blockiness will be more disturbing compared with the same amount of blockiness in videos with less temporal motion. Hence, the distortion needs to be masked according to the motion present in the video sequence. In this paper, we discuss different methods to apply temporal masking by employing the available motion vectors. The motion vectors, which contain the temporal activity of each frame, are first extracted from the H.264 decoder. We illustrate that motion vectors alone are not enough to estimate the motion present in the sequence. Hence we also use the standard deviation for motion masking. Temporal masking is applied in No Reference mode because in many applications it is not possible to obtain the complete reference video in order to obtain the video quality. It is also time consuming to process both reference and coded video sequences in full and reduced reference modes. The designed algorithm is tested on LIVE video database. The database includes the subjective score for each video sequence. A correlation coefficient of 88.21% is achieved when tested with our video quality meter. \n","\n","Manuscript deals with introduction of model for automatic selection of the scene with the best position of the object of interest in a multi-camera live broadcasts. The novel metric for evaluation of object appearance in multi-camera scenes was proposed and designed following the deep analysis of relevant broadcasting technologies and object tracking methods. Evaluation of object appearance in scene comprises not only from the location and size of the object of interest in the actual frame but also from streaming transmission parameters and the subjective rating from the broadcast recipients. The proposed metrics serve as the basis for the establishment of a system for selecting the best scene with switching in the real-time. Model has been experimentally deployed in two alternative implementations using common and mobile devices. Results were compared with human based broadcast direction. Based on this comparison, the ability to respond to changes in the scene and also to capture the object of interest in the stream was observed. The resulting application of model should be adapted in different fields such as broadcast of conferences, sport events or security systems.\n","\n","To date, online shopping using e-commerce services becomes a trend. The emergence of e-commerce truly helps people to shop more effectively and efficiently. However, there are still some problems encountered in e-commerce, especially from the user perspective. This research aims to explore user review data, particularly on factors that influence user perception of e-commerce applications, classify, and identify potential solutions to finding problems in e-commerce applications. Data is grabbed using web scraping techniques and classified using proper machine learning, i.e., support vector machine (SVM). Text associations and fishbone analysis are performed based on the classified user review data. The results of this study show that the user satisfaction problem can be captured. Furthermore, various services that should be provided as a potential solution to experienced customers' problems or application users' perception problems can be generated. A detailed discussion of these findings is available in this article. \n","\n","Writing music interaction systems is not easy because their concurrent processes usually access shared resources in a non-deterministic order, often leading to unpredictable behavior. Using Pure Data (Pure Data) and Max/MSP, it is possible to program concurrency; however, it is difficult to synchronize processes based on multiple criteria. Process calculi such as the Non-deterministic Timed Concurrent Constraint (ntcc) calculus, overcome that problem by representing, declaratively, the synchronization of multiple criteria as constraints. In this article, we propose the framework Ntccrt, as a new alternative to manage concurrency in Pure Data and Max/MSP. Ntccrt is a real-time capable interpreter for ntcc. Using Ntccrt binary plugins in Pure Data, we executed models for machine improvisation and signal processing. We also analyzed two case studies: one of a machine improvisation system and one of a signal processing system. We found out that performance of both case studies is compatible with soft real-time music interaction; it means, a musician can interact with Ntccrt without noticeable delays during the interaction. \n","\n","Combining first order logic rules with a Knowledge Graph (KG) embedding model has recently gained increasing attention, as rules introduce rich background information. Among such studies, models equipped with soft rules, which are extracted with certain confidences, achieve state-of-the-art performance. However, the existing methods either cannot support the transitivity and composition rules or take soft rules as regularization terms to constrain derived facts, which is incapable of encoding the logical background knowledge about facts contained in soft rules. In addition, previous works performed one time logical inference over rules to generate valid groundings for modeling rules, ignoring forward chaining inference, which can further generate more valid groundings to better model rules. To these ends, this paper proposes Soft Logical rules enhanced Embedding (SoLE), a novel KG embedding model equipped with a joint training algorithm over soft rules and KG facts to inject the logical background knowledge of rules into embeddings, as well as forward chaining inference over rules. Evaluations on Freebase and DBpedia show that SoLE not only achieves improvements of 11.6%/5.9% in Mean Reciprocal Rank (MRR) and 18.4%/15.9% in HITS@1 compared to the model on which SoLE is based, but also significantly and consistently outperforms the state-of-the-art baselines in the link prediction task. \n","\n","With the growing demand for high safety in industrial system, fault diagnosis has attracted more and more attention. Currently, belief rule base (BRB) has shown an excellent performance in modeling complex system, where the expert knowledge is used effectively. Existing BRB models are assumed that the inputs of the attributes are independent and the attribute correlation is not taken into account. However, in some engineering system, there is an obvious correlation among these attributes. The correlated attributes may produce redundant information which limits the abilities of attributes to express the accurate information of system. In this paper, a new BRB model with considering attribute correlation (BRB-c) is proposed. Moreover, a decoupling matrix is introduced to eliminate the redundant information from the attributes. The initial parameters of the decoupling matrix are given according to the expert knowledge. And then, when the inputs of the attributes are available, the parameters in the decoupling matrix are trained by an optimization model. The projection covariance matrix adaption evolution strategy is chosen as an optimization algorithm. A practical case study about fault diagnosis of oil pipeline is conducted and the results show that the BRB-c model can diagnose the leak size and leak time of oil pipeline accurately, which can demonstrate that the proposed model can be widely applied in engineering for fault diagnosis. \n","\n","We describe the clustered low-rank (CLR) framework for block-sparse and block-low-rank tensor representation and computation. The CLR framework exploits the tensor structure revealed by basis clustering; computational savings arise from low-rank compression of tensor blocks and performing block arithmetic in the low-rank form whenever beneficial. The precision is rigorously controlled by two parameters, avoiding ad-hoc heuristics, such as domains: one controls the CLR block rank truncation, and the other controls screening of small contributions in arithmetic operations on CLR tensors to propagate sparsity through expressions. As these parameters approach zero, the CLR representation and arithmetic become exact. As a pilot application, we considered the use of the CLR format for the order-2 and order-3 tensors in the context of the density fitting (DF) evaluation of the Hartree-Fock (exact) exchange (DF-K). Even for small systems and realistic basis sets, CLR-DF-K becomes more efficient than the standard DF-K approach, and it has significantly reduced asymptotic storage and computational complexities relative to the standard O(N3) and O(N4) DF-K figures. CLR-DF-K is also significantly more efficient-all while negligibly affecting molecular energies and properties-than the conventional (non-DF) O(N) exchange algorithm for applications to medium-sized systems (on the order of 100 atoms) with diffuse Gaussian basis sets, a necessity for applications to negatively charged species, molecular properties, and high-accuracy correlated wave functions. \n","\n","The design of good host overload/underload detection and virtual machine (VM) placement algorithms plays a vital role in assuring the smoothness of VM live migration. The presence of the dynamic environment that leads to a changing load on the VMs motivates us to propose a Markov prediction model to forecast the future load state of the host. We propose a host load detection algorithm to find the future overutilized/underutilized hosts state to avoid immediate VMs migration. Moreover, we propose a VM placement algorithm to determine the set of candidates hosts to receive the migrated VMs in a way to reduce their VM migrations in near future. We evaluate our proposed algorithms through CloudSim simulation on different types of PlanetLab real and random workloads. The experimental results show that our proposed algorithms have a significant reduction in terms of service-level agreement violation, the number of VM migrations, and other metrics than the other competitive algorithms. \n","\n","In this paper, we study the approximation properties of some exponential and semi-exponential operators. We focus on modifications of these operators in King’s sense, examining the rate of convergence for basic and modified operators. The presented line of reasoning emphasizes some symmetry in the modifications of exponential and semi-exponential operators. \n","\n","In this paper we propose a novel approach to hybrid visual steering of simulation ensembles. A simulation ensemble is a collection of simulation runs of the same simulation model using different sets of control parameters. Complex engineering systems have very large parameter spaces so a naïve sampling can result in prohibitively large simulation ensembles. Interactive steering of simulation ensembles provides the means to select relevant points in a multi-dimensional parameter space (design of experiment). Interactive steering efficiently reduces the number of simulation runs needed by coupling simulation and visualization and allowing a user to request new simulations on the fly. As system complexity grows, a pure interactive solution is not always sufficient. The new approach of hybrid steering combines interactive visual steering with automatic optimization. Hybrid steering allows a domain expert to interactively (in a visualization) select data points in an iterative manner, approximate the values in a continuous region of the simulation space (by regression) and automatically find the 'best' points in this continuous region based on the specified constraints and objectives (by optimization). We argue that with the full spectrum of optimization options, the steering process can be improved substantially. We describe an integrated system consisting of a simulation, a visualization, and an optimization component. We also describe typical tasks and propose an interactive analysis workflow for complex engineering systems. We demonstrate our approach on a case study from automotive industry, the optimization of a hydraulic circuit in a high pressure common rail Diesel injection system. \n","\n","We propose a novel progressive framework to optimize deep neural networks. The idea is to try to combine the stability of linear methods and the ability of learning complex and abstract internal representations of deep learning methods. We insert a linear loss layer between the input layer and the first hidden non-linear layer of a traditional deep model. The loss objective for optimization is a weighted sum of linear loss of the added new layer and non-linear loss of the last output layer. We modify the model structure of deep canonical correlation analysis (DCCA), i.e., adding a third semantic view to regularize text and image pairs and embedding the structure into our framework, for cross-modal retrieval tasks such as text-to-image search and image-to-text search. The experimental results show the performance of the modified model is better than similar state-of-art approaches on a dataset of National University of Singapore (NUS-WIDE). To validate the generalization ability of our framework, we apply our framework to RankNet, a ranking model optimized by stochastic gradient descent. Our method outperforms RankNet and converges more quickly, which indicates our progressive framework could provide a better and faster solution for deep neural networks. \n","\n","In this paper, it proposed overall design, database design and security control design program of Web2.0-based multimedia work management system by demand analysis. The design of multimedia work management system blended into the idea of Web 2.0. It advocated to utilize collective intelligence with network power, and made users a content viewer and manufacturer of system content. System together with user built a exchange platform with harmonious social relations.\n","\n","In last 10 years, several noise reduction (NR) algorithms have been proposed to be combined with the blind source separation techniques to separate speech and noise signals from blind noisy observations. More often, techniques use voice activity detector (VAD) systems for the optimal solution. In this paper, we propose a new backward blind source separation (BBSS) structure that uses the input correlation properties to provide: (i) high convergence rates and good tracking capabilities, since the acoustic environments imply long and time-variant noise paths, and (ii) low misalignment and robustness against different noise type variations and double-talk. The proposed algorithm has an automatic behavior to enhance noisy speech signals, and do not need any VAD systems to separate speech and noise signals. The obtained results in terms of several objective criteria show the good performance properties of the proposed algorithm in comparison with state-of-the-art algorithms. \n","\n","The H∞ sliding mode control is studied for a class of singular Markovian jump systems with sensor failure, incompleted transition probabilities and randomly changing structure. A quantized input sliding mode controller is designed to deal with the sensor failure and time-varying delay. A new sliding mode surface and a new Lyapunov function are constructed to achieve the system stable and reduce the effect of the jumping mode. Based on the linear matrix inequalities (LMIs) theory, some sufficient conditions are derived to guarantee stochastically admissible with performance γ. The sliding mode reachability is also guaranteed. Simulation results are given to illustrate the effectiveness of the proposed method in this paper. \n","\n","In this paper, a first systematic review and analysis of the current state of the art pertaining to complexity problems for a heavily researched class of designs, namely covering arrays, is presented. In addition to surveying all known complexity results for covering arrays, we formalize many informal notions and related concepts of the related literature. Last but not least, we also correct a number of misinterpretations and false statements which have appeared in the literature, regarding the NP-hardness of some problems related to covering arrays. Respectively, we update the current state of the art, enriching it also with our own results. \n","\n","We present a formal methodology for identifying a channel in a system consisting of a communication channel in cascade with an asynchronous sampler. The channel is modeled as a multidimensional filter, while models of asynchronous samplers are taken from neuroscience and communications and include integrate-and-fire neurons, asynchronous sigma/delta modulators and general oscillators in cascade with zero-crossing detectors. We devise channel identification algorithms that recover a projection of the filter(s) onto a space of input signals loss-free for both scalar and vector-valued test signals. The test signals are modeled as elements of a reproducing kernel Hilbert space (RKHS) with a Dirichlet kernel. Under appropriate limiting conditions on the bandwidth and the order of the test signal space, the filter projection converges to the impulse response of the filter. We show that our results hold for a wide class of RKHSs, including the space of finite-energy bandlimited signals. We also extend our channel identification results to noisy circuits. \n","\n","This article proposes a Serious Game about 'Set-Based Concurrent Engineering', which is one of the elements of lean practices in product design and development. Although Set-Based Concurrent Engineering is becoming popular in academia, in practice, understanding and adoption of it are low. Thus, the game presented in this article is designed to bring hands-on experience to practitioners to understand its principles and associated enablers. The game is structured in two stages, simulating the traditional approach to concept development, called 'Point-Based Concurrent Engineering', and the lean approach, called Set-Based Concurrent Engineering, respectively. Performance metrics are provided in the game to track teams' performances in the two stages. Several practitioners have played the game. This article also presents the feedback obtained from a game session to illustrate the educational purposes and effectiveness of the game. \n","\n","Modern malware often evades debuggers, virtual machines, and emulators. It is interesting if one can observe their behavior difference using controlled environments. This paper formalizes the notion of environment sensitivity, and proposes two alternative semantics: one based on program trace and the other based on code coverage. Then it tackles the following question: can one minimize an environment sensitive program? The work presents progressive executable slice, a subprogram generated from a partial control but full data dependency closure of a program under study. It shows that a progressive slice can retain trace based environment sensitivity but not the code coverage sensitivity, for which, a special condition is needed for restraining the slice. The saturated trace set of a program is used as a cost indicator for observing its behavior difference. The paper shows that a progressive slice does not necessarily have a lower observation cost than its container program. To address it, a consistency condition is proposed. The paper introduces a reference algorithm for generating progressive slices, and discusses its approximation in practice. \n","\n","Air, an essential natural resource, has been compromised in terms of quality by economic activities. Considerable research has been devoted to predicting instances of poor air quality, but most studies are limited by insufficient longitudinal data, making it difficult to account for seasonal and other factors. Several prediction models have been developed using an 11-year dataset collected by Taiwan’s Environmental Protection Administration (EPA). Machine learning methods, including adaptive boosting (AdaBoost), artificial neural network (ANN), random forest, stacking ensemble, and support vector machine (SVM), produce promising results for air quality index (AQI) level predictions. A series of experiments, using datasets for three different regions to obtain the best prediction performance from the stacking ensemble, AdaBoost, and random forest, found the stacking ensemble delivers consistently superior performance for R2 and RMSE, while AdaBoost provides best results for MAE. \n","\n","This paper considers the vehicle video detection and tracking problem. We propose a method based on the partitioning a video into blocks of equal length. Vehicles are detected in the first and last frames of each block. After that, locations of the object in the intermediate frames can be recovered. Experimental results show above 70% true positive detection rate; the average false positives per frame is 0.3. \n","\n","Economic dispatch (ED) is one of the most important optimization problems in a power system. The objective of ED is sharing the power demand among the online generators while keeping the minimum cost of generation as a constraint. The aim of this paper is to operate an electric power system as economically as possible within its security limits. This paper proposes the following 2 new particle swarm optimization (PSO) algorithms to solve a nonconvex economic dispatch problem: an efficient PSO is termed as efficient particle swarm optimization (EPSO), and a hybrid of evolutionary programming (EP) and EPSO is termed as EP-EPSO. Since ED was introduced, several methods have been used to solve these problems. However, none of these methods can provide an optimal solution because they become trapped at some local optima. Stochastic optimization techniques such as EPSO and EP have the advantage of a good convergent property. A significant speed-up can be obtained by the hybrid of these algorithms. The proposed techniques are tested on standard test systems available in the literature. The performance of the proposed EP-EPSO is compared with a) biogeography-based optimization, b) adaptive particle swarm optimization, c) the genetic algorithm, d) a 2-phase neural network, e) PSO with time-varying acceleration coefficients, f ) NEW-PSO, and g) differential evolution with biogeography-based optimization. It is observed that the EP-EPSO has a higher convergence rate, advanced quality, and better optimal cost when compared to the other techniques. The considered ED problems have been solved, including transmission losses without valve-point loading effects. \n","\n","Big data is the term for a collection of datasets so huge and complex that it becomes difficult to be processed using on-hand theoretical models and technique tools. Brain big data is one of the most typical, important big data collected using powerful equipments of functional magnetic resonance imaging, multichannel electroencephalography, magnetoencephalography, Positron emission tomography, near infrared spectroscopic imaging, as well as other various devices. Granular computing with multiple granular layers, referred to as multi-granular computing (MGrC) for short hereafter, is an emerging computing paradigm of information processing, which simulates the multi-granular intelligent thinking model of human brain. It concerns the processing of complex information entities called information granules, which arise in the process of data abstraction and derivation of information and even knowledge from data. This paper analyzes three basic mechanisms of MGrC, namely granularity optimization, granularity conversion, and multi-granularity joint computation, and discusses the potential of introducing MGrC into intelligent processing of brain big data. \n","\n","Cloud computing can provide Virtual Machine (VM) computing resources to meet the growing computational demands. In this paper, we propose a scheduling strategy based on an Improved Genetic Algorithm (IGA) for independent task in Cloud Computing environment. Especially, the IGA scheduling strategy can shorten the completion time of the tasks with priority as well as total completion time. Our scheduling strategy based on IGA performs better than other original scheduling strategies such as the Random, Rotating and Greedy in open-source platform, which have been proved by the simulation results in the CloudSim toolkit. It also shows the IGA is better robust than the standard GA when the load and computing power are extremely unbalanced. \n","\n","Speculative execution attacks present an enormous security threat, capable of reading arbitrary program data under malicious speculation, and later exfiltrating that data over microarchitectural covert channels. This paper proposes speculative taint tracking (STT), a high security and high performance hardware mechanism to block these attacks. The main idea is that it is safe to execute and selectively forward the results of speculative instructions that read secrets, as long as we can prove that the forwarded results do not reach potential covert channels. The technical core of the paper is a new abstraction to help identify all micro-architectural covert channels, and an architecture to quickly identify when a covert channel is no longer a threat. We further conduct a detailed formal analysis on the scheme in a companion document. When evaluated on SPEC06 workloads, STT incurs 8.5% or 14.5% performance overhead relative to an insecure machine. \n","\n","We present a novel and efficient method to compute volumetric soft shadows for interactive direct volume visualization to improve the perception of spatial depth. By direct control of the softness of volumetric shadows, disturbing visual patterns due to hard shadows can be avoided and users can adapt the illumination to their personal and application-specific requirements. We compute the shadowing of a point in the data set by employing spatial filtering of the optical depth over a finite area patch pointing toward each light source. Conceptually, the area patch spans a volumetric region that is sampled with shadow rays; afterward, the resulting optical depth values are convolved with a low-pass filter on the patch. In the numerical computation, however, to avoid expensive shadow ray marching, we show how to align and set up summed area tables for both directional and point light sources. Once computed, the summed area tables enable efficient evaluation of soft shadows for each point in constant time without shadow ray marching and the softness of the shadows can be controlled interactively. We integrated our method in a GPU-based volume renderer with ray casting from the camera, which offers interactive control of the transfer function, light source positions, and viewpoint, for both static and time-dependent data sets. Our results demonstrate the benefit of soft shadows for visualization to achieve user-controlled illumination with many-point lighting setups for improved perception combined with high rendering speed. \n","\n","Automatic segmentation of the epidermis area in skin histopathological images is an essential step for computer-aided diagnosis of various skin cancers. This paper presents a robust technique for epidermis segmentation in the whole slide skin histopathological images. The proposed technique first performs a coarse epidermis segmentation using global thresholding and shape analysis. The epidermis thickness is then measured by a series of line segments perpendicular to the main axis of the initially segmented epidermis mask. If the segmented epidermis mask has a thickness greater than a predefined threshold, the segmentation is assumed to be inaccurate. A second pass of fine segmentation using k-means algorithm is then carried out over these coarsely segmented result to enhance the performance. Experimental results on 64 different skin histopathological images show that the proposed technique provides a superior performance compared to the existing techniques. \n","\n","Meeting application requirements under a tight power budget is of a primary importance to enable connected health internet of things applications. This paper considers using sparse representation and well-defined inequality indexes drawn from the theory of inequality to distinguish ventricular ectopic beats (VEBs) from non-VEBs. Our approach involves designing a separate dictionary for each arrhythmia class using a set of labeled training QRS complexes. Sparse representation, based on the designed dictionaries of each new test QRS complex is then calculated. Following this, its class is predicted using the winner-Takes-All principle by selecting the class with the highest inequality index. The experiments showed promising results ranging between 80% and 100% for the detection of VEBs considering the patient-specific approach, 80% using cross validation and 70% on unseen data using independent sets for training and testing, respectively. An efficient hardware implementation of the alternating direction method of multipliers algorithm is also presented. The results show that the proposed hardware implementation can classify a QRS complex in 69.3ms that use only 0.934 W energy. \n","\n","This paper combines data-driven and model-driven methods for real-Time misinformation detection. Our algorithm, named Quick-Stop, is an optimal stopping algorithm based on a probabilistic information spreading model obtained from labeled data. The algorithm consists of an offline machine learning algorithm for learning the probabilistic information spreading model and an online optimal stopping algorithm to detect misinformation. The online detection algorithm has both low computational and memory complexities. Our numerical evaluations with a real-world dataset show that QuickStop outperforms existing misinformation detection algorithms in terms of both accuracy and detection time (number of observations needed for detection). Our evaluations with synthetic data further show that QuickStop is robust to (offline) learning errors. \n","\n","Clustering algorithms are usually based on the Bag-of-Words (BOW) approach. A tarnished hindrance of the BOW prototypical is that it ignores the semantic relationship among words. As a result, if two documents use different collections of core words to represent the same topic, they may be assigned to different clusters even though the core words they use are probably synonyms or semantically associated in other form and other disadvantage of conventional web page clustering technique is often utilized to reveal the functional similarity of WebPages. Tagging can be beneficial to improve the clustering performance. Several efforts have been made to explore social tagging for clustering. But there is some drawbacks of tagging web based clustering. All the existing approaches exploiting tag information for web page clustering assume that all the WebPages are tagged which is a somewhat restrictive assumption. In a more realistic setting, one can only expect that the tags will be available for only a small number of WebPages. Researchers propose a new web page grouping approach based on Probabilistic Latent Semantic Analysis (PLSA) Model. An iterative set of rules based on maximum likelihood principle is employed to overcome the aforementioned computational shortcoming. \n","\n","Survival analysis methods that integrate pathways/gene sets into their learning model could identify molecular mechanisms that determine survival characteristics of patients. Rather than first picking the predictive pathways/gene sets from a given collection and then training a predictive model on the subset of genomic features mapped to these selected pathways/gene sets, we developed a novel machine learning algorithm (Path2Surv) that conjointly performs these two steps using multiple kernel learning. Results: We extensively tested our Path2Surv algorithm on 7655 patients from 20 cancer types using cancer-specific pathway/gene set collections and gene expression profiles of these patients. Path2Surv statistically significantly outperformed survival random forest (RF) on 12 out of 20 datasets and obtained comparable predictive performance against survival support vector machine (SVM) using significantly fewer gene expression features (i.e. less than 10% of what survival RF and survival SVM used). \n","\n","In neuroscience large amounts of data are recorded to provide insights into cerebral information processing and function. The successful extraction of the relevant signals becomes more and more challenging due to increasing complexities in acquisition techniques and questions addressed. Here, automated signal processing and machine learning tools can help to process the data, e.g., to separate signal and noise. With the presented software pySPACE (http://pyspace.github.io/pyspace), signal processing algorithms can be compared and applied automatically on time series data, either with the aim of finding a suitable preprocessing, or of training supervised algorithms to classify the data. pySPACE originally has been built to process multi-sensor windowed time series data, like event-related potentials from the electroencephalogram (EEG). The software provides automated data handling, distributed processing, modular build-up of signal processing chains and tools for visualization and performance evaluation. Included in the software are various algorithms like temporal and spatial filters, feature generation and selection, classification algorithms, and evaluation schemes. Further, interfaces to other signal processing tools are provided and, since pySPACE is a modular framework, it can be extended with new algorithms according to individual needs. In the presented work, the structural hierarchies are described. It is illustrated how users and developers can interface the software and execute offline and online modes. Configuration of pySPACE is realized with the YAML format, so that programming skills are not mandatory for usage. The concept of pySPACE is to have one comprehensive tool that can be used to perform complete signal processing and classification tasks. It further allows to define own algorithms, or to integrate and use already existing libraries. \n","\n","Testing is the best way to ensure software quality. However, this activity makes a huge challenge about limited time and financial resources. In order to achieve high quality, managers need to detect the defect prone parts of code and allocate the resources to them. Although many metrics, techniques and models have been proposed, effective identification of such parts is still a challenge. In this paper two new metrics, namely PIEDG and PIMDG are proposed for predicting defects based on dependency between the file level components of the code. The proposed metrics use the notion of participation of nodes in the software dependency graph to recognize the parts of the code which possess defects. This research also investigates the effect of ego graph on the software dependency global graph. The feasibility of the software defect predictor which is built based on the proposed metrics is evaluated. The experiments over the Eclipse bug dataset demonstrate promising results in anticipation of the software defects. \n","\n","The paper is motivated by the task of cross-lingual standard document clustering. This paper presents a novel model, called entity based cross-lingual standard document clustering model, to conduct cross-lingual document clustering. While most existing cross-lingual document clustering methods make use of an expensive machine translation system to fill the gap between two languages, our model aims to effectively handle the cross-lingual document clustering by conducting coreference resolution on documents and linking the cross-lingual entities in a compact way. Experimental results show our method, without any resources other than a bilingual dictionary can achieve comparable performance to the direct machine translation method via a machine translation system, e.g. Google language tool. Also, our method is more efficient.\n","\n","Bearings are commonly used machine elements and an important part of mechanical transmission. They are widely used in automobiles, airplanes, and various instruments and equipment. Bearing rollers are the most important components in a bearing and determine the performance, life, and stability of the bearing. In order to control the surface quality of the rollers, a machine vision system for bearing roller surface inspection is proposed. We briefly introduced the design of the machine vision system and then focused on the surface inspection algorithm. We proposed a multi-task convolutional neural network to detect defects. We extracted the features of the defects through a shared convolutional neural network, then classified the defects and calculated the position of the defects simultaneously. Finally, we determined if the bearing roller was qualified according to the position, category, and area of the defect. In addition, we explored various factors affecting performance and conducted a large number of experiments. We compared our method with the traditional methods and proved that our method had good stability and robustness. \n","\n","Knowledge management system (KMS) support the activities of knowledge management (KM) in early warning system (EWS) to provide early warning and aid in decision facilitation. Both KMS with EWS are also applicable in the clinical diagnostic (CD) environment during the CD activities to provide early warning and aid in decision facilitation of disease outbreak such as dengue. However, lack of proper data or information management and limited knowledge sharing and dissemination within the organization is a main challenge for mitigation of risk. In which the problem that may relate to this challenge is the timeliness for timely reporting and decision facilitation during CD activities. Therefore, a conceptual of KMS model with EWS in CD environment of dengue fever is formulated based on the existing components of KM, KMS with EWS. Then, the components of CD activities and dengue fever are also identified and studied for the model implementation. A pre survey and the analysis of five existing previous models were carried out to determine the significant components of KMS and EWS. The pre survey results analysis and gaps drawn from the analysis of these five models are used as a basis to the initial proposed of KMS model with EWS in CD environment of dengue fever. Next, the KMS model with EWS was then validated via prototype to verify the model reliability and is evaluated via a post survey. The model which is the integration between KMS with EWS is known as KMS with EWS is to enable the capturing, storing, reusing and managing of knowledge in order to provide early warning and aid in decision facilitation of dengue fever. \n","\n","New technologies, such as social networks, wikis, blogs and other social tools, enable collaborative work and are important facilitators of the social learning process. Many companies are using these types of tools as substitutes for their intranets, especially software development companies. However, the content generated by these tools in many cases is not appropriately organized. Therefore, this information is often not accessed by the company. Learning objects and units of learning are two e-learning concepts that allow content to be organized in a suitable sequence, thus improving its learning and reuse. Therefore, an approach is proposed to generate learning objects and units of learning from social tools in order to organize information for easy reuse. To evaluate the proposed approach, an experimental study was conducted and subjected to discursive textual analysis. The results show that the approach is viable for improving organizational learning in software development teams. Furthermore, the approach is efficient, especially in terms of the acquisition of new knowledge. It also helps to maintain the organizational pattern and minimize the reinvention of solutions and the repetition of errors. \n","\n","The stochastic block model (SBM) is a random graph model in which the edges are generated according to the underlying cluster structure on the vertices. The (ferromagnetic) Ising model, on the other hand, assigns ±1 labels to vertices according to an underlying graph structure in a way that if two vertices are connected in the graph then they are more likely to be assigned the same label. In SBM, one aims to recover the underlying clusters from the graph structure while in Ising model, an extensively-studied problem is to recover the underlying graph structure based on i.i.d. samples (labelings of the vertices). In this paper, we propose a natural composition of SBM and the Ising model, which we call the Stochastic Ising Block Model (SIBM). In SIBM, we take SBM in its simplest form, where n vertices are divided into two equal-sized clusters and the edges are connected independently with probability p within clusters and q across clusters. Then we use the graph G generated by the SBM as the underlying graph of the Ising model and draw m i.i.d. samples from it. The objective is to exactly recover the two clusters in SBM from the samples generated by the Ising model, without observing the graph G. As the main result of this paper, we establish a sharp threshold m*on the sample complexity of this exact recovery problem in a properly chosen regime, where m*can be calculated from the parameters of SIBM. We show that when m\\ge m*, one can recover the clusters from m samples in O(n) time as the number of vertices n goes to infinity. When m < m*, we further show that for almost all choices of parameters of SIBM, the success probability of any recovery algorithms approaches 0 as n → ∞. \n","\n","This paper explores the idea of knowledge-based security policies, which are used to decide whether to answer queries over secret data based on an estimation of the querier's (possibly increased) knowledge given the results. Limiting knowledge is the goal of existing information release policies that employ mechanisms such as noising, anonymization, and redaction. Knowledge-based policies are more general: they increase flexibility by not fixing the means to restrict information flow. We enforce a knowledge-based policy by explicitly tracking a model of a querier's belief about secret data, represented as a probability distribution, and denying any query that could increase knowledge above a given threshold. We implement query analysis and belief tracking via abstract interpretation, which allows us to trade off precision and performance through the use of abstraction. We have developed an approach to augment standard abstract domains to include probabilities, and thus define distributions. We focus on developing probabilistic polyhedra in particular, to support numeric programs. While probabilistic abstract interpretation has been considered before, our domain is the first whose design supports sound conditioning, which is required to ensure that estimates of a querier's knowledge are accurate. Experiments with our implementation show that several useful queries can be handled efficiently, particularly compared to exact (i.e., sound) inference involving sampling. We also show that, for our benchmarks, restricting constraints to octagons or intervals, rather than full polyhedra, can dramatically improve performance while incurring little to no loss in precision. \n","\n","Scatterplots are widely used to visualize scatter dataset for exploring outliers, clusters, local trends, and correlations. Depicting multi-class scattered points within a single scatterplot view, however, may suffer from heavy overdraw, making it inefficient for data analysis. This paper presents a new visual abstraction scheme that employs a hierarchical multi-class sampling technique to show a feature-preserving simplification. To enhance the density contrast, the colors of multiple classes are optimized by taking the multi-class point distributions into account. We design a visual exploration system that supports visual inspection and quantitative analysis from different perspectives. We have applied our system to several challenging datasets, and the results demonstrate the efficiency of our approach. \n","\n","This paper presents a comprehensive comparative study for the tracking control of a class of underactuated nonlinear uncertain systems. A given nonlinear model of the underactuated system is, at first stage, transformed into an input output form and the driving applied control input of the transformed system is then designed via four sliding mode control strategies, i.e., conventional first order sliding mode control, second order sliding mode, fast terminal sliding mode, and integral sliding mode. At second stage, a ball and beam system is considered and the aforementioned four control design strategies are experimentally implemented. A comprehensive comparative study of the simulation and experimental results is then conducted which take into account the tracking performance, i.e., settling time, overshoots, robustness enhancement, chattering reduction, sliding mode convergences, and control efforts. \n","\n","Cancer is one of the major causes of death in humans. Early diagnosis of genetic mutations that cause cancer tumor growth leads to personalized medicine to the decease and can save the life of majority of patients. With this aim, Kaggle has conducted a competition to classify clinically actionable gene mutations based on clinical evidence and some other features related to gene mutations. The dataset contains 3321 training data points that can be classified into 9 classes. In this work, an attempt is made to classify these data points using K-nearest neighbors (KNN) and linear support vector machines (SVM) in a multi class environment. As the features are categorical, one hot encoding as well as response coding are applied to make them suitable to the classifiers. The prediction performance is evaluated using log loss and KNN has performed better with a log loss value of 1.10 compared to that of SVM 1.24. \n","\n","The paper studies the argumentation-based negotiation technology of multi-agent, improves the multi-agent communication model by combining argumentation and minimum spanning tree algorithm, and designs the algorithm about selecting optimal path of center agent, that raises the adaptive ability and work efficiency of the model. Copyright \n","\n","Principal Component Analysis is a multivariate method to project data in a reduced hyperspace, defined by orthogonal principal components, which are linear combinations of the original variables. In this way, data dimension can be reduced, noise can be excluded from the subsequent analysis, and therefore, data interpretation is extremely facilitated. For these reasons, Principal Component Analysis is nowadays the most common chemometric strategy for unsupervised exploratory data analysis.In this paper, the PCA toolbox for MATLAB is described. This is a collection of modules for calculating Principal Component Analysis, as well as Cluster Analysis and Multidimensional Scaling, which are two other well-known multivariate methods for unsupervised data exploration. The toolbox is freely available via Internet and comprises a graphical user interface (GUI), which allows the calculation in an easy-to-use graphical environment. It aims to be useful for both beginners and advanced users. The use of the toolbox is discussed here with an appropriate practical example. \n","\n","In this paper an integrated method that brings together the image features form color, shape and texture is proposed for content based image retrieval (CBIR). The present paper converts the color image in to HSV color space and derives color histograms. The V color space of the image is divided into non overlapping region of size 9x9. Each region is sub divided into nine non overlapped sub-regions and a feature vector is derived on each sub region. These feature vectors of each sub region compresses the 9x9 multiregion into 3x3. To derive shape feature, textons are computed on each 2x2 grid and the image is converted into “multi region texton matrix” (MRTM). The gray level co-occurrence matrix (GLCM) features are derived on MRTM and the image retrieval is performed on five categories of Wang database images by combining color histograms and the GLCM features of MRTM. The proposed method of CBIR is compared with GLCM and texton co-occurrence method (TCM) and results indicates the efficacy of the proposed method. \n","\n","Distributed Denial of Services (DDoS) attacks continue to be one of the most challenging threats to the Internet. The intensity and frequency of these attacks are increasing at an alarming rate. With the promising results presented by Machine Learning (ML) techniques in variety fields, researchers have proposed numerous intelligent schemes to defend against DDoS attacks and mitigate their impact. This paper presents a taxonomy of the ML-based DDoS detection schemes, focusing on the important features and mechanisms that each scheme uses to detect and mitigate the impact of these attacks. The taxonomy is developed based on a thorough and extensive review of the literature, focusing on the most prominent and highly cited schemes that have been proposed over the last decade. The taxonomy is then used as a basis for the development of a framework to conduct a comprehensive empirical evaluation of the basic mechanisms underling the design of the selected ML-based DDoS defense schemes against a variety of attack scenarios. Rather than dealing with the specific details of a particular DDoS defense scheme, this work focuses on the “building blocks” of the intelligent DDoS detection and prevention schemes. The intelligent mechanisms underlying the selected schemes are implemented and evaluated using different performance metrics. The impact of different influential factors are also explored, including the observable traffic proportions, attack intensities and the “Class Imbalance Problem” of ML-based DDoS detection. The results of the comparative analysis show that no single technique outperforms all others in all test cases. Furthermore, the results underscore the need for a method oriented feature selection model to enhance the capabilities of ML-based detection techniques. Finally, the results show that the class imbalance problem significantly impact performance, underscoring the need for further research to address this problem and ensure high-quality DDoS detection in real-time. \n","\n","The concept of information plays a fundamental role in our everyday experience, but is conspicuously absent in framework of classical physics. Over the last century, quantum theory and a series of other developments in physics and related subjects have brought the concept of information and the interface between an agent and the physical world into increasing prominence. As a result, over the last few decades, there has arisen a growing belief amongst many physicists that the concept of information may have a critical role to play in our understanding of the workings of the physical world, both in more deeply understanding existing physical theories and in formulating of new theories. In this paper, I describe the origin of the informational view of physics, illustrate some of the work inspired by this view, and give some indication of its implications for the development of a new conception of physical reality. \n","\n","Light stimulation with precise and complex spatial and temporal modulation is demanded by a series of research fields like visual neuroscience, optogenetics, ophthalmology, and visual psychophysics. We developed a user-friendly and flexible stimulus generating framework (GEARS GPU-based Eye And Retina Stimulation Software), which offers access to GPU computing power, and allows interactive modification of stimulus parameters during experiments. Furthermore, it has built-in support for driving external equipment, as well as for synchronization tasks, via USB ports. The use of GEARS does not require elaborate programming skills. The necessary scripting is visually aided by an intuitive interface, while the details of the underlying software and hardware components remain hidden. Internally, the software is a C++/Python hybrid using OpenGL graphics. Computations are performedon the GPU, andare defined in the GLSL shading language. However, all GPU settings, including the GPU shader programs, are automatically generated by GEARS. This is configured through a method encountered in game programming, which allows high flexibility: stimuli are straightforwardly composed using a broad library of basic components. Stimulus rendering is implemented solely in C++, therefore intermediary libraries for interfacing could be omitted. This enables the program to perform computationally demanding tasks like en-masse random number generation or real-time image processing by local and global operations. \n","\n","We present FluxFlow, an interactive visual analysis system for revealing and analyzing anomalous information spreading in social media. Everyday, millions of messages are created, commented, and shared by people on social media websites, such as Twitter and Facebook. This provides valuable data for researchers and practitioners in many application domains, such as marketing, to inform decision-making. Distilling valuable social signals from the huge crowd's messages, however, is challenging, due to the heterogeneous and dynamic crowd behaviors. The challenge is rooted in data analysts' capability of discerning the anomalous information behaviors, such as the spreading of rumors or misinformation, from the rest that are more conventional patterns, such as popular topics and newsworthy events, in a timely fashion. FluxFlow incorporates advanced machine learning algorithms to detect anomalies, and offers a set of novel visualization designs for presenting the detected threads for deeper analysis. We evaluated FluxFlow with real datasets containing the Twitter feeds captured during significant events such as Hurricane Sandy. Through quantitative measurements of the algorithmic performance and qualitative interviews with domain experts, the results show that the back-end anomaly detection model is effective in identifying anomalous retweeting threads, and its front-end interactive visualizations are intuitive and useful for analysts to discover insights in data and comprehend the underlying analytical model. \n","\n","Today computers anchored its potential in various tasks of image processing. Due to its rapid growth various methods has been established to resolve different issues which ranges from the past to the present. The common task in image processing is the retrieval of images based on its contents. In earlier CBIR schemes major process has achieve by binary or gray scale images and such research steps nearer to its destination in terms of its accuracy. Hence, it motivates another channel to enter in the doors of colour image retrieval. It is now marching with various methodologies and shrinking with several issues. To overwhelm such challenges in this area of retrieval, we propose a novel image retrieval of color images based on its components using Principal Component Analysis (PCA) model. First, the input color image is separated its RGB component into each single component. Then using the PCA the image matching is accomplished with the one component process, then the two component process respectively. This proposed component based image retrieval provides better in many single components for matching.. \n","\n","Information and communication technologies (ICT) have changed our daily lives and have particularly influenced the field of education, revolutionizing the means of teaching and learning. This article focuses mainly on the blended learning, whose on-site, lessons are devoted to the essential matter offered to the learners; and the ICT are used for a deep learning. We will use the collective intelligence that is shared on social websites such as Facebook in order to create learning groups. To make this objective possible, we will base our work on the generative probabilistic model of Latent Dirichlet Allocation. We will use this model on all the discussions shared between learners and between learners and teachers to classify students according to the topics and the difficulties that each one of them has expressed; in order to help the teachers build new knowledge on the degree of assimilation of students. \n","\n","Verification is an important part of the chip design process. Design is usually represented in hardware description language (HDL). Contemporary HDLs have constructs that are characteristic to software programs. Therefore, the methods to automatically generate test for software programs can be applied to generate test for HDL models. One of such methods is symbolic execution. We present a framework to generate test benches for HDL models. The framework combines the methods of symbolic execution and control flow graph, which are usually used in the context of software programs, with finite state machine that is characteristic for HDL models. The framework is implemented in Python programming language. We experimented with ITC'99 benchmark suite and compared the performance of our framework with similar research. Our obtained results outperformed the results taken from similar research.\n","\n","Elastic network models (ENMs) are valuable tools for investigating collective motions of proteins, and a rich variety of simple models have been proposed over the past decade. A good representation of the collective motions requires a good approximation of the covariances between the fluctuations of the individual atoms. Nevertheless, most studies have validated such models only by the magnitudes of the single-atom fluctuations they predict. In the present study, we have quantified the agreement between the covariance structure predicted by molecular dynamics (MD) simulations and those predicted by a representative selection of proposed coarse-grained ENMs. We then contrast this approach with the comparison to MD-predicted atomic fluctuations and comparison to crystallographic B-factors. While all the ENMs yield approximations to the MD-predicted covariance structure, we report large and consistent differences between proposed models. We also find that the ability of the ENMs to predict atomic fluctuations is correlated with their ability to capture the covariance structure. In contrast, we find that the models that agree best with B-factors model collective motions less reliably and recommend against using B-factors as a benchmark. \n","\n","K-harmonic means (KHM) is a centroid based clustering algorithm, which will easily converge to local optimal and optimize single objective function. Recent researches have shown that there is no single cluster validity index works equally well for Diffierent kinds of datasets. Multiobjective clustering is used to solve this problem, which optimizes multiple validity measures simultaneously. Besides Levy ight Cuckoo Search is a recent developed nature inspired meta-heuristic algorithm that works efficiently for clustering. These facts motivate us to propose a novel multiobjective KHM clustering algorithm using Levy Flight Cuckoo Search (MOKHMCS) which optimizes the KHM objective function and Xie-Beni index simultaneously. Some modifications have been made in Cuckoo Search to apply to MOKHMCS. It will produce a near-Pareto optimal non-dominated set of solutions in the final generation and PBMF index is used to select a final solution in the set. The experimental results on two artificial and UCI datasets and three bioinformatics datasets indicate the superiority of MOKHMCS compared to KHM, Cuckoo Search based KHM and Particle Swarm Optimization based multiobjective KHM. The results also show the effectiveness of multiobjective clustering and the efficiency of Levy ight Cuckoo Search helps KHM escape from local optimal and speed up convergence. Copyright \n","\n","The significance and reasons of key problems including the simulating efficiency, the model systems, the standard specification and system application are analyzed. On this basis, we discuss how to use techniques of high performance simulation engine, system modeling focused on C4ISR, artificial intelligence, and warfighting experimentation to solve these problems. We propose “two-line & three-layer” model VV&A management, “three-in-one” cooperative working pattern, and standard operation process of simulation analysis. Our works provide references for construction and application of the future joint operations simulation system. \n","\n","Today's students, being used to constant activity and multitasking in their everyday life, need a high level of social and creative engagement in order to learn; for them, highly interactive learning environments which allow for communica-tion, collaboration, and authoring, are a must. In addition, modern learning theories stress the importance of interactivity and engagement of students for successful learning processes, whereas recent empirical studies provide evidence and confirm this. In this paper, we present how the integration of Social and Semantic Web technologies, often referred to as the Social Semantic Web (SSW), along with the Linked Data paradigm offer potentials for improving the interactivity of today's learn-ing environments, while putting students in control of their learning process spanning across different tools and services. We identify the main principles on which such SSW-supported personal learning environments are based, and illustrate them through the design, implementation, analysis, and evaluation of DEPTHS (DEsign Patterns Teaching Help System) -a SSW-based interactive personal learning environment we have developed for the domain of software design patterns. \n","\n","Human action recognition plays a vital role in surveillance applications. Human action recognition is motivated by some of the applications such as video retrieval, video surveillance systems, human robot interaction, to interact with deaf and dumb people etc. The aim is to analyse the role of Adaboost in the process of recognising the human action by extracting the motion features using optical flow. Adaboost is a supervised learning method used to select the subset of frames with most discriminatory motion features. Saliency point computation is performed to assign a measure of interest to each visual unit. Mean shift algorithm is then used for tracking the objects. Gabor feature is the global feature that includes more detailed information of frequency and orientation. Haar feature is used to show the variation in the pixel. Relevance vector machine classification gives a probabilistic output through Bayesian inference. The proposed system reduces the computation time and provides a higher recognition rate in comparison with existing gentle boost-based recognition system. Copyright \n","\n","This paper evaluates the sequential performance of virtual execution environments (VEE) belonging to the CLI (Common Language Infrastructure) and JVM (Java Virtual Machine) standards, for the usual approaches of representing multidimensional arrays in programs that present intensive array access. Such programs are mostly found in scientific and engineering application domains. It shows that the performance profile of virtual execution is still highly influenced by the choice of both the multidimensional array representation and the VEE implementation, also contradicting some results of previous related works, as well as some beliefs of HPC programmers that come from their practice with native execution languages, such as C, C++, and Fortran. Finally, recommendations are provided both for HPC programmers that desire to take advantage of VEE performance for their array-intensive code and for VEE developers that are interested in improving the performance of virtual execution for the needs of HPC applications. \n","\n","The combination of spectroscopy and a novel classification algorithm called extreme learning machine (ELM) was developed for food classification. The performance influence of different input dimensionalities was also investigated. The classification accuracy and speed of five chemometrics techniques, including k-nearest neighbor (KNN), partial least-squares discriminant analysis (PLS-DA), back propagation artificial neural network (BP-ANN), least-squares support vector machine (LS-SVM), and ELM were compared. It was presented that the accuracy of ELM was better than its competitors in most cases. Moreover, on the classification stage, ELM performed much faster than KNN, LS-SVM, and BP-ANN, which indicated that ELM may be a promising method for real-time food classification with a comparable accuracy based on near or mid-infrared spectroscopy. \n","\n","Neuromorphic computing using neural network hardware has attracted significant interest as it promises improved performance at low power for data-intensive error-resilient graphical signal processing. Oscillatory neural networks (ONNs) use either frequency or phase as state variables to implement frequency-shift keying (FSK)- and phase-shift keying (PSK)-based neural networks, respectively. To make these ONNs power and area efficient, back-end-of-the-line compatible, and capable of processing multilevel information, we explore an emerging class of oscillators that show fine-grain frequency-tuning and phase-coupling. We examine TaOx- and TiOx-based oscillators (resistive random access memory-type) as elements of a neuromorphic compute block and experimentally demonstrate: 1) frequency control over four decades using a ballast MOSFET; 2) variable phase coupling between oscillators; and 3) variable phase programming between oscillators coupled with a MOSFET. Such fine-grain control over both frequency and relative phase serve as the desirable characteristics of oscillators required for multilevel information processing in star-type directly coupled FSK- and PSK-based neuromorphic systems that find applications in gray-scale image processing and other graphical compute paradigms. These attributes combined with the small size (&lt;1 μm2) and simplicity, make these devices attractive candidates for realizing large-scale neuromorphic systems at reasonable size and power. \n","\n","Interactions in cyberspace are an essential element of daily life today, as people and the systems that they use support email, social media, electronic commerce, automated decision-making and other services that benefit people in their private lives and in their work. However, great harm also occurs, through crime and fraud, identity theft, the spreading of misinformation, and the violating of ethical principles surrounding civil society. Trust refers to the degree of belief that people and systems act dependably, reliably, and securely in the context in which they operate. Trust is a social construct with deep roots in personal interactions among people. When extended into the digital world of today, the concept of trust broadly involves groups of people, entire societies and governments, and the platforms and systems that they use. We describe data gathering and models that apply to interactions among people, devices, machines, computational platforms, and intelligent decision-support systems, and ways in which trust can be quantitatively modeled and measured. The concept of identity management is discussed in relation to trust as a personal asset of an actor. A new graph-theoretic trust model that can function within blockchain environments is described, and analytical results are applied using available data for a Bitcoin community. Comparisons are made with competing trust models, potential for blockchain technologies to support trust as an asset of an individual entity. \n","\n","Motivated by the success of using black-box predictive algorithms as subroutines for online decision-making, we develop a new framework for designing online policies given access to an oracle providing statistical information about an offline benchmark. Having access to such prediction oracles enables simple and natural Bayesian selection policies, and raises the question as to how these policies perform in different settings. Our work makes two important contributions towards tackling this question: First, we develop a general technique we call compensated coupling which can be used to derive bounds on the expected regret (i.e., additive loss with respect to a benchmark) for any online policy and offline benchmark; Second, using this technique, we show that the Bayes Selector has constant expected regret (i.e., independent of the number of arrivals and resource levels) in any online packing and matching problem with a finite type-space. Our results generalize and simplify many existing results for online packing and matching problems, and suggest a promising pathway for obtaining oracle-driven policies for other online decision-making settings. \n","\n","This study deals with the chaotic phenomenon of nonlinear Chua's circuit for power generator systems. Takagi–Sugeno (T–S) fuzzy model of a nonlinear system is established. By constructing a suitable Lyapunov functional, exponential stability conditions are obtained for fuzzy systems. Based on the sampled-data control theory, extreme sensitivity is visualised in the state trajectory depending on the initial conditions and sampled-data fuzzy controllers are designed in the form of linear matrix inequality (LMI). Finally, some numerical simulation results are shown that the sampled-data fuzzy control system adopts a well-designed methodology. \n","\n","The compressive strength of concrete is an important process in constructions arrangement and proportioning new mixtures which demonstrates its level of quality assurance. In this research, a concrete strength prediction model has been developed based on Fuzzy approach. This model will estimate the compressive strength of a concrete based on 1030 sample of ready-mix concrete based on its design mix constituents, namely cement, blast furnace slag, fly ash, water, superplasticizer, coarse aggregate, and age. Four different model are compared with each other using statistical performance. The experimental results show that Type-2 fuzzy model gives minimum errors for predicting compressive strength of concrete. \n","\n","Context-aware access control systems should reactively adapt access control decisions to dynamic environmental conditions. In this paper we present ERBAC - an event-driven extension of the TRBAC model that allows the specification and enforcement of general reactive policies - and its implementation. While almost all the individual features of ERBAC occur separately in some previous model, the detailed design of the policy language, its implementation in XACML, and its testing contribute to the development of expressive, event-driven policy frameworks by demonstrating that this rich model can be satisfactorily implemented, and that its expressivity and performance are compatible with a variety of realistic application scenarios. In particular, a number of examples illustrate ERBAC's expressive power, and its ability of handling exceptional situations in a flexible way, while keeping policies compact and manageable. The prototype extends XACML's language and the implementation of the PDP to support the new model. Systematic scalability experiments show that the computational cost of policy rule evaluation in ERBAC is compatible with real-world applications. \n","\n","Pandemic is a multi-player board game which simulates the outbreak of epidemics and the human effort to prevent them. It is a characteristic of this game that all the players cooperate for a goal and they are not competitive. We show that the problem to decide if the player can win the generalized Pandemic from the given situation of the game is NP-complete. \n","\n","Over the last years, numerous ICT applications with mechanisms to detect situations have been developed to support disaster management (DM), which is a field of a great societal and economic importance. Those applications are termed situation-aware (SA) because they try, in near real-time, to perceive and comprehend a situation of some type (e.g. disease epidemics) and project a reaction to the detected situation (e.g. isolate diseased people). An obstacle to the modelling of SA applications is the lack of well-founded structural and temporal constructs, which is inherent to conventional design techniques. Ontology-driven conceptual modelling has been successfully applied to overcome this issue, where ontological analysis based on a foundational ontology supports the modelling of concepts within a specific field as a well-founded core ontology. In this paper we discuss the importance of a well-founded core ontology for DM to support the specification of SA applications. We give an overview of the comprehensive framework we are developing, in which the DM core ontology plays a prominent role in the development of SA applications. In particular, we discuss the challenge of harmonizing concepts related to the modelling of situations in a foundational ontology in the lights of the Barwisean situation theory, Situoid theory and Situation awareness theory. This challenge has to be addressed to properly support SA applications in DM. \n","\n","The increasing complexity of embedded systems in information and communication technology causes a problem with locating faults during system failures. One reason for this problem is that system components that receive abnormal input data from other components may also output abnormal data, even if they are not in abnormal states, and consequently many redundant faults are detected in the system. In this paper, we present a diagnosis method for locating the origin of faults automatically in systems where fault propagation may occur. We use a model-based diagnosis scheme and abstract behavior modeling technique to deal with complex software components. We propose a new approach to diagnose systems that have data flow loops. Finally, we propose a one-stage approach for solving the abstract model-based diagnosis based on its formulation into the partial maximum satisfiability problem. \n","\n","Axiomatic characterizations of approximation operators are important in the study of rough set theory. In this paper, axiomatic characterizations of relation-based fuzzy rough approximation operators determined by a fuzzy implication operator I are investigated. We first review the constructive definitions and properties of lower and upper I-fuzzy rough approximation operators. We then propose an operator-oriented characterization of I-fuzzy rough sets. We show that the lower and upper I-fuzzy rough approximation operators generated by an arbitrary fuzzy relation can be described by single axioms. We further examine that I-fuzzy rough approximation operators corresponding to some special types of fuzzy relations, such as serial, reflexive, and T-transitive ones, can also be characterized by single axioms.\n","\n","A monaural speech separation/enhancement technique based on non-negative tucker decomposition (NTD) has been introduced in this paper. In the proposed work, the effect of sparsity regularization factor on the separation of mixed signal is included in the generalized cost function of NTD. By using the proposed algorithm, the vector components of both target and mixed signal can be exploited and used for the separation of any monaural mixture. Experiment was done on the monaural data generated by mixing the speech signals from two speakers and, by mixing noise and speech signals using TIMIT and noisex-92 dataset. The separation results are compared with the other existing algorithms in terms of correlation of separated signal with the original signal, signal to distortion ratio, perceptual evaluation of speech quality and short-time objective intelligibility. Further, to get more conclusive information about separation ability, speech recognition using Kaldi toolkit was also performed. The recognition results are compared in terms of word error rate (WER) using the MFCC based features. Results show the average improved WER using proposed algorithm over the nearest performing algorithm is up to 2.7% for mixed speech of two speakers and 1.52% for noisy speech input. \n","\n","How can the result of a query be updated after changing a database? This is a fundamental task for database management systems which ideally takes previously computed information into account. In dynamic complexity theory, it is studied from a theoretical perspective where updates are specified by rules written in first-order logic. In this article we sketch recent techniques and results from dynamic complexity theory with a focus on the reachability query. \n","\n","With the increasing domain and widespread use of wireless devices in recent years (mobile phones, Internet of Things, Wi-Fi), the electromagnetic spectrum has become extremely crowded. To counter security threats posed by rogue or unknown transmitters, we must identify RF transmitters not only by the data content of the transmissions but also based on the intrinsic physical characteristics of the transmitters. RF waveforms represent a particular challenge because of the extremely high data rates involved and the potentially large number of transmitters sharing a channel in a given location. These factors outline the need for rapid fingerprinting and identification methods that go beyond the traditional hand-engineered approaches. In this paper, we investigate the use of machine learning strategies to the classification and identification problem. We evaluate four different strategies: Conventional deep neural nets, convolutional neural nets, support vector machines, and deep neural nets with multi-stage training. The latter was by far the most accurate, achieving 100% classification accuracy of 12 transmitters, and showing remarkable potential for scalability to large transmitter populations. \n","\n","Digital images play an inevitable role in human life and hence, the utilization of images grow day-by-day. Though the advanced storage technology helps in massive data storage, efficient retrieval system is the need of this hour and this issue is well-addressed by Content Based Image Retrieval (CBIR) systems. The CBIR systems are widely present for healthcare and remote sensing domain. However, the presence of CBIR systems is found to be limited for fabric images. Taking this as a challenge, this work presents a CBIR system exclusively meant for fabric images by extracting color and texture features. When the user passes the search query image to the CBIR system, the features of the query image is compared with the features of the images in the dataset, which is performed by ensemble classification. The performance of the proposed CBIR system is found to be satisfactory in terms of retrieval accuracy and time consumption. \n","\n","Balancing the workload of sophisticated simulations is inherently difficult, since we have to balance both computational workload and memory footprint over meshes that can change any time or yield unpredictable cost per mesh entity, while modern supercomputers and their interconnects start to exhibit fluctuating performance. We propose a novel lightweight balancing technique for MPI+X to accompany traditional, prediction-based load balancing. It is a reactive diffusion approach that uses online measurements of MPI idle time to migrate tasks temporarily from overloaded to underemployed ranks. Tasks are deployed to ranks which otherwise would wait, processed with high priority, and made available to the overloaded ranks again. This migration is nonpersistent. Our approach hijacks idle time to do meaningful work and is totally nonblocking, asynchronous and distributed without a global data view. Tests with a seismic simulation code developed in the ExaHyPE engine uncover the method's potential. We found speed-ups of up to 2-3 for ill-balanced scenarios without logical modifications of the code base and show that the strategy is capable to react quickly to temporarily changing workload or node performance. \n","\n","Recognition systems using multimodal biometrics attracts attention because they improve recognition efficiency and high-security level compared to the unimodal biometrics system. In this study, the authors present a secure multimodal biometrics recognition system based on the deep learning method that uses convolutional neural networks (CNNs). The authors propose two multimodal architectures using the finger knuckle print (FKP) and the finger vein (FV) biometrics with different levels of fusion: The features level fusion and scores level fusion. The features extraction for FKP and FV are performed using transfer learning CNN architectures: AlexNet, VGG16, and ResNet50. The key step aims to select separate features descriptors from each unimodal biometrics modality. After that, the authors combine them using the proposed fusion approaches were support vector machine or Softmax applies as classifiers to increase the proposed system security. The efficiency of the proposed algorithms is tested using publicly available biometrics databases. The experimental results show that the proposed fusion architectures achieve an accuracy of 99.89% and an equal error rate of 0.05%. The obtained results indicate that the biometrics recognition system using deep learning is secure, robust, and reliable. \n","\n","Data used in machine learning applications is prone to contain both vague and incomplete information. Many authors have proposed to use fuzzy rough set theory in the development of new techniques tackling these characteristics. Fuzzy sets deal with vague data, while rough sets allow to model incomplete information. As such, the hybrid setting of the two paradigms is an ideal candidate tool to confront the separate challenges. In this paper, we present a thorough review on the use of fuzzy rough sets in machine learning applications. We recall their integration in preprocessing methods and consider learning algorithms in the supervised, unsupervised and semi-supervised domains and outline future challenges. Throughout the paper, we highlight the interaction between theoretical advances on fuzzy rough sets and practical machine learning tools that take advantage of them.\n","\n","The transparency order is proposed as a parameter for the robustness of S-boxes to differential power analysis (DPA): lower transparency order implying more resistance. However, most cryptographically strong S-boxes have been found to have high transparency order. In this paper, we characterize transparency order for various classes of S-boxes by computing the upper and lower bounds of transparency order for both even and odd numbers of variables. We find high transparency order values in the class of S-boxes whose sum of autocorrelation spectra of the coordinate functions has zero value for a large number of vectors a. Also instead of propagation characteristics, autocorrelation spectra of the S-box function F are found to be stronger in deciding the transparency order. With this characterization, we performed a constrained random generation and search of a class of balanced 8\\,\\times\\,8 S-boxes with transparency order upper bounded by 7.8. The nonlinearity and absolute indicator values of global avalanche characteristics of the coordinate functions of the S-boxes are in the range (98, 110) and (48, 88), respectively. A correlation analysis DPA on table look-up implementation of AES Rijndael S-box revealed the last round key in 700 power traces, while it took at least 1500 power traces with S-boxes from our proposed class. \n","\n","A query over RDF data is usually expressed in terms of matching between a graph representing the target and a huge graph representing the source. Unfortunately, graph matching is typically performed in terms of subgraph isomorphism, which makes semantic data querying a hard problem. In this paper we illustrate a novel technique for querying RDF data in which the answers are built by combining paths of the underlying data graph that align with paths specified by the query. The approach is approximate and generates the combinations of the paths that best align with the query. We show that, in this way, the complexity of the overall process is significantly reduced and verify experimentally that our framework exhibits an excellent behavior with respect to other approaches in terms of both efficiency and effectiveness. \n","\n","The interaction of users with search services has been recognized as an important mechanism for expressing and handling user information needs. One traditional approach for supporting such interactive search relies on exploiting relevance feedbacks (RF) in the searching process. For large-scale multimedia collections, however, the user efforts required in RF search sessions is considerable. In this paper, we address this issue by proposing a novel semi-supervised approach for implementing RF-based search services. In our approach, supervised learning is performed taking advantage of relevance labels provided by users. Later, an unsupervised learning step is performed with the objective of extracting useful information from the intrinsic dataset structure. Furthermore, our hybrid learning approach considers feedbacks of different users, in collaborative image retrieval (CIR) scenarios. In these scenarios, the relationships among the feedbacks provided by different users are exploited, further reducing the collective efforts. Conducted experiments involving shape, color, and texture datasets demonstrate the effectiveness of the proposed approach. Similar results are also observed in experiments considering multimodal image retrieval tasks. \n","\n","Graphs resulting from human behavior (the web graph, friendship graphs, etc.) have hitherto been viewed as a monolithic class of graphs with similar characteristics; for instance, their degree distributions are markedly heavy tailed. In this paper we take our understanding of behavioral graphs a step further by showing that an intriguing empirical property of web graphs - their compressibility - cannot be exhibited by well-known graph models for the web and for social networks. We then develop a more nuanced model for web graphs and show that it does exhibit compressibility, in addition to previously modeled web graph properties. \n","\n","The design of hybrid metaheuristics with ideas taken from the simulated annealing and evolutionary algorithms fields is a fruitful research line. In this paper, we first present an overview of the hybrid metaheuristics based on simulated annealing and evolutionary algorithms presented in the literature and classify them according to two well-known taxonomies of hybrid methods. Second, we perform an empirical study comparing the behavior of a representative set of the hybrid approaches based on evolutionary algorithms and simulated annealing found in the literature. In addition, a study of the synergy relationships provided by these hybrid approaches is presented. Finally, we analyze the behavior of the best performing hybrid metaheuristic with regard to several state-of-the-art evolutionary algorithms for binary combinatorial problems. The experimental studies presented provide useful conclusions about the schemes for combining ideas from simulated annealing and evolutionary algorithms that may improve the performance of these kinds of approaches and suggest that these hybrids metaheuristics represent a competitive alternative for binary combinatorial problems. \n","\n","The malfunctioning of the heating, ventilating, and air conditioning (HVAC) system is considered to be one of the main challenges in modern buildings. Due to the complexity of the building management system (BMS) with operational data input from a large number of sensors used in HVAC system, the faults can be very difficult to detect in the early stage. While numerous fault detection and diagnosis (FDD) methods with the use of statistical modeling and machine learning have revealed prominent results in recent years, early detection remains a challenging task since many current approaches are unfeasible for diagnosing some HVAC faults and have accuracy performance issues. In view of this, this study presents a novel hybrid FDD approach by combining random forest (RF) and support vector machine (SVM) classifiers for the application of FDD for the HVAC system. Experimental results demonstrate that our proposed hybrid random forest–support vector machine (HRF–SVM) outperforms other methods with higher prediction accuracy (98%), despite that the fault symptoms were insignificant. Furthermore, the proposed framework can reduce the significant number of sensors required and work well with the small number of faulty training data samples available in real-world applications. \n","\n","The universum samples, which do not belong to any class of the current interest, have been proved helpful for semi-supervised classification (SSC). It actually opens up a new learning problem called semi-supervised universum classification (SSUC), which simultaneously utilizes the labeled, unlabeled and universum data for learning. However, the existing works for SSUC all utilize the universum samples in graph-based SSC methods, which belong to a branch for SSC based on the manifold assumption, and learn in the transductive style. In this paper, we concentrate on another important branch for SSC based on the cluster assumption, and attempt to utilize the universum samples for further enhancing the performance in the inductive learning style. Finally we develop a novel inductive SSUC method based on the cluster assumption. Empirical experiments on the USPS and MNIST datasets show the superiority of the proposed method compared with both the SSC and SSUC methods. Copyright \n","\n","The age of Big Data has inspired the appearance and application of data-intensive Web service. In most cases, multiple data-intensive services are assembled into a service composition to meet complicated requirements. As the number of Data-intensive Web services on the Internet is increasing rapidly and dramatically, traditional central service composition approaches have come to a performance bottleneck. This paper proposes a method for automatic data-intensive service composition, which can be executed in parallel. Firstly, the problem of automatic service composition is defined by combing the approaches of State Space Search and Planning Graph. Then, a heuristic algorithm is proposed to compose data-intensive Web service automatically. After that, more details are given to present the parallel optimization for the composition algorithm. A series of experiments show that the proposed parallel optimization method improves the efficiency of automatic Web service composition to a great extent.\n","\n","Case-control studies are important and useful methods for studying health outcomes and many methods have been developed for analyzing case-control data. Those methods, however, are vulnerable to mismeasurement of variables; biased results are often produced if such a feature is ignored. In this paper, we develop an inference method for handling case-control data with interacting misclassified covariates. We use the prospective logistic regression model to feature the development of the disease. To characterize the misclassification process, we consider a practical situation where replicated measurements of error-prone covariates are available. Our work is motivated in part by a breast cancer case-control study where two binary covariates are subject to misclassification. Extensions to other settings are outlined. \n","\n","Voice activity detection (VAD) refers to the task of identifying vocal segments from an audio clip. It helps in reducing the computational overhead as well elevate the recognition performance of speech-based systems by helping to discard the non vocal portions from an input signal. In this paper, a VAD technique is presented that uses line spectral frequency-based statistical features namely LSF-S coupled with extreme learning-based classification. The experiments were performed on a database of more than 350 h consisting of data from multifarious sources. We have obtained an encouraging overall accuracy of 99.43%. \n","\n","Brain tumor is one of the health problems faced by human beings. It often leads to death of people. Detecting it early can help in taking treatment and improve quality of life. The detection has to be made with MRI brain tumor images. Fourier transform, wavelet transform, Ridgelet transform and Curvelet transform are the techniques exist for representing images. Fourier transform can represent signals with only frequency domain and information on temporal domain is missing. To overcome this drawback, wavelet transform is used which can represent signal using wavelets in both time and frequency domains. However, wavelets are not good for images with different angles and different scales. Ridgelets could handle images with line singularities but could not handle images with curves. Curvlet transform can overcome this problem besides representing images with different scales and different angles. Curvlet Transform (CT) with enhancements can support dynamic texture classification for detection of brain tumor. Thus in this paper Enhanced CT (ECT) is used to have better diagnosis of brain tumor. A framework with underlying algorithms based on ECT is designed and implemented. A prototype application is built using MATLAB to demonstrate proof of the concept. The empirical results revealed that the proposed method has significant performance improvement over state of the art approaches. \n","\n","Although the synthesis problem is often undecidable for distributed, synchronous systems, it becomes decidable for the subclass of uniformly well-connected (UWC) architectures, provided that only robust specifications are considered. It is then an important issue to be able to decide whether a given architecture falls in this class. This is the problem addressed in this paper: we establish the decidability and precise complexity of checking this property. This problem is in EXPSPACE and NP-hard in the general case, but falls into PSPACE when restricted to a natural subclass of architectures. \n","\n","Software-defined network (SDN)-based vehicular ad hoc network (VANET) is an outstanding technology for smart transportation as it increases traffic safety, efficiency, comfort, and manageability. However, despite all its benefits and good performance, SDN-based VANET is vulnerable to attack threats such as distributed denial of service (DDoS). When SDN-based VANET systems are exposed to DDoS attacks, this may affect traffic safety, causing traffic accidents and deaths. Therefore, the relevant security threats need to be addressed before integrating the SDN-based VANETs into smart transportation systems. In this study, the stacked sparse autoencoder (SSAE) + Softmax classifier deep network model is proposed to detect DDoS attacks targeting SDN-based VANETs. The features in the dataset obtained from the SDN-based VANET were reduced dimensionally utilising SSAE, and the most significant features were obtained. Then, these features were used as input into the Softmax classifier. According to the experimental results, the best accuracy scores were calculated as 96.9% using the four-layer SSAE + Softmax classifier deep network model proposed. When compared, the results demonstrate the SSAE + Softmax classifier deep network model proposed can obtain better results in the classification of DDoS attacks and is more successful than the other machine learning classifiers. \n","\n","An algorithm is proposed that combines nonlinear feature generation and sparse regression to learn interpretable nonlinear models from noisy and limited data. This Algebraic Learning Via Elastic Net for Static and Dynamic Nonlinear Model Identification algorithm employs automated feature generation including families of ubiquitous chemical and biological nonlinear transformations. ALVEN balances model complexity and prediction accuracy through a two-step feature selection procedure, to produce an interpretable model useful for process applications while avoiding overfitting. The generalization to nonlinear dynamical systems, Dynamic ALVEN, is then described. The model accuracy of the algorithms is compared to well-established machine learning methods for a 3D printer and a chemical reactor. \n","\n","In this paper, a Particle Swarm Optimization proposition is presented to solve the Homography problem. We tested our technique with several datasets. Also a comparison with existing technique (SVD) is done. PSO shows a significant superiority comparing to classic approach (SVD). The main gain of this technique is the accuracy of results and easiness of implementation. \n","\n","Parenthetical translations are translations of terms in otherwise monolingual text that appear inside parentheses. Parenthetical translations extraction (PTE) is the task of extracting parenthetical translations from natural language documents. One of the main difficulties in PTE is to detect the left boundary of the translated term in preparenthetical text. In this article, we propose a collective approach that employs Markov logic to model multiple constraints used in the PTE task. We show how various constraints can be formulated and combined in a Markov logic network (MLN). Our experimental results show that the proposed collective PTE approach significantly outperforms a current state-of-the-art method, improving the average F-measure up to 27.11% compared to the previous word alignment approach. It also outperforms an individual MLN-based system by 8.2% and a system based on conditional random fields by 5.9%. \n","\n","In this paper we present a generic algorithmic framework, namely, the accelerated stochastic approximation (AC-SA) algorithm, for solving strongly convex stochastic composite optimization (SCO) problems. While the classical stochastic approximation algorithms are asymptotically optimal for solving differentiable and strongly convex problems, the AC-SA algorithm, when employed with proper stepsize policies, can achieve optimal or nearly optimal rates of convergence for solving different classes of SCO problems during a given number of iterations. Moreover, we investigate these AC-SA algorithms in more detail, such as by establishing the large-deviation results associated with the convergence rates and introducing an efficient validation procedure to check the accuracy of the generated solutions. \n","\n","We present a substructural epistemic logic, based on Boolean BI, in which the epistemic modalities are parametrized on agents' local resources. The new modalities can be seen as generalizations of the usual epistemic modalities. The logic combines Boolean BI's resource semantics - we introduce BI and its resource semantics at some length - with epistemic agency. We illustrate the use of the logic in systems modelling by discussing some examples about access control, including semaphores, using resource tokens. We also give a labelled tableaux calculus and establish soundness and completeness with respect to the resource semantics. \n","\n"]}],"source":["cnt = 0\n","abstracts_to_show = 100\n","abstracts_shown = 0\n","for idx,prediction in enumerate(predictions_subj_sort):\n","    if subj_sorted[idx]=='Computer_Science' and prediction>=0.5:\n","        if(cnt>=0 and abstracts_shown<abstracts_to_show):\n","            print(test_set[0][subject_sort_idx[idx]])\n","            print(\"\")\n","            abstracts_shown+=1\n","        cnt+=1\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["#create excel with abstracts predicted as AI per subject area\n","current_subj = subj_sorted[0]\n","abstracts_list = []\n","writer = pd.ExcelWriter('AI_Predictions_per_Subject_Area.xlsx', engine = 'openpyxl')\n","\n","for idx,prediction in enumerate(predictions_subj_sort):\n","    if subj_sorted[idx] != current_subj:\n","        df = pd.DataFrame(abstracts_list)\n","        df.to_excel(writer, sheet_name = current_subj)\n","        writer.save()\n","        abstracts_list = []\n","        current_subj = subj_sorted[idx]\n","    \n","    if(prediction>=0.5):\n","        abstracts_list.append(test_set[0][subject_sort_idx[idx]])\n","\n","writer.close()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["writer = pd.ExcelWriter('AI_Paper_Classification_Dataset2.xlsx', engine = 'openpyxl')\n","df = pd.DataFrame(test_set[0])\n","df.to_excel(writer, sheet_name = '1', encoding='utf-8', index=False)\n","writer.save()\n","writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4gSxsu9aP0f"},"outputs":[],"source":["from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n","\n","bert_tokenizer_params=dict(lower_case=True)\n","reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", '[CLS]', '[SEP]', '[MASK]']\n","\n","bert_vocab_args = dict(\n","    # The target vocabulary size\n","    vocab_size = 20000,\n","    # Reserved tokens that must be included in the vocabulary\n","    reserved_tokens=reserved_tokens,\n","    # Arguments for `text.BertTokenizer`\n","    bert_tokenizer_params=bert_tokenizer_params,\n","    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n","    learn_params={},\n",")\n","\n","dataset2 = tf.data.Dataset.from_tensor_slices(dataset2)\n","\n","pt_vocab = bert_vocab.bert_vocab_from_dataset(\n","    dataset2.batch(1000).prefetch(2),\n","    **bert_vocab_args\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1637751956615,"user":{"displayName":"Stefan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiRhdkMdo0hF2RWdgdNNaT_7ppLHw6YT3SGEzUmIQ=s64","userId":"07000003532239847619"},"user_tz":-120},"id":"e2YeiStVk2MK","outputId":"0354b927-570e-4058-83c5-8eb93a6cd8ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '£', '¥', '¦', '§', '©', '¬', '®', '°', '±', '·', '»', '¿', '×', 'æ', 'ð', '÷', 'ø', 'þ', 'đ', 'ħ', 'ı', 'ĸ', 'ł', 'œ', 'ɑ', 'ɛ', 'ɤ', 'ʹ']\n","['ʼ', 'ˆ', 'α', 'β', 'γ', 'δ', 'ε', 'ζ', 'η', 'θ', 'ι', 'κ', 'λ', 'μ', 'ν', 'ξ', 'ο', 'π', 'ρ', 'σ', 'τ', 'υ', 'φ', 'χ', 'ψ', 'ω', 'ϭ', 'ћ', 'ԑ', '‐', '‒', '–', '—', '‖', '‘', '’', '“', '”', '„', '†', '‡', '•', '‰', '′', '‹', '›', '⁄', '⁎', '€', '℘', '→', '↔', '⇒', '∀', '∂', '∃', '∅', '∆', '∇', '∈', '∏', '∑', '−', '∓', '∕', '∗', '∘', '∙', '√', '∝', '∞', '∣', '∥', '∧', '∨', '∩', '∪', '∫', '∶', '∼', '≃', '≈', '≊', '≔', '≡', '≤', '≥', '≧', '≪', '≫', '≲', '≳', '⊂', '⊃', '⊆', '⊕', '⊖', '⊗', '⊘', '⊙']\n","['⊥', '⋅', '⋆', '⋯', '⌈', '⌉', '⌊', '⌋', '⌐', '⍺', '□', '◊', '◦', '☓', '♀', '♂', '⟨', '⟩', '⩾', '〈', '〉', '漢', '葦', '語', '\\ue34c', '\\uf104', '\\uf106', '�', 'the', 'of', 'and', 'in', 'to', 'for', 'is', 'with', '##s', 'that', 'we', 'this', 'are', 'by', 'on', 'as', 'from', 'an', 'be', 'were', 'was', 'which', 'or', 'data', 'at', 'results', 'these', 'can', 'using', 'based', 'model', 'have', 'between', 'study', '##ing', 'it', 'not', 'has', 'their', 'our', 'used', 'all', 'two', '##ly', 'methods', '##d', '##ed', 'learning', 'been', 'time', 'analysis', 'also', 'network', 'patients', 'neural', 'such', 'use', 'more', 'but', 'author', '##a', 'both', 'paper', 'health', '##e', 'one', 'high', 'different', 'system', 'studies', 'than', 'models', 'associated', 'new', 'may', 'show', 'other', 'its', 'research', '##t', 'will', 'however', 'information', 'risk', 'method', 'networks', 'proposed', 'into', 'approach', 'algorithm', 'non', 'cells', '##n', 'disease', 'well', 'most', 'number', '##2', 'first', '##r', 'cell', 'control', 'clinical', 'during', 'there', 'human', 'when', 'how', '##i', '##1', '95', 'compared', 'over', 'problem', 'performance', 'function', 'ltd', 'each', '##al', 'here', 'within', 'background', 'large', '##c', 'systems', 'treatment', 'specific', 'some', 'through', 'under', '##l', 'care', 'only', 'three', '##m', 'no', 'including', 'important', 'ci', 'evidence', '##3', 'level', 'al', 'low', '##es', 'present', '##o', '##y', 'et', '##p', 'years', '##er', 'state', 'set', 'provide', 'many', 'potential', 'they', 'where', 'development', 'while', 'group', 'after', 'activity', 'malaria', 'significant', 'problems', 'effects', 'population', 'process', 'participants', 'found']\n","['##⩾', '##〈', '##〉', '##漢', '##葦', '##語', '##\\ue34c', '##\\uf104', '##\\uf106', '##�']\n"]}],"source":["print(pt_vocab[:100])\n","print(pt_vocab[100:200])\n","print(pt_vocab[200:400])\n","print(pt_vocab[-10:])"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["No_Computer_Science.xlsx\n","No_Chemical_Engineering.xlsx\n","No_Mathematics.xlsx\n","No_Neuroscience.xlsx\n","No_Engineering.xlsx\n","No_Chemistry.xlsx\n","No_Material_Science.xlsx\n","No_dentistry.xlsx\n","No_Social_Sciences.xlsx\n","No_Agricultural_and_Biological_Sciences.xlsx\n","No_Physics_and_Astronomy.xlsx\n","No_Immunology_and_Microbiology.xlsx\n","No_Environmental_Sciences.xlsx\n","No_economics_econometrics.xlsx\n","No_Psychology.xlsx\n","No_Biochemistry, Genetics and Molecular Biology.xlsx\n","No_Health_professions.xlsx\n","No_Earth_and_Planetary_sciences.xlsx\n","No_Pharmacology_Toxicology,Pharmaceutics.xlsx\n","No_Business_management_accounting.xlsx\n","No_Nursing.xlsx\n","No_decision_science.xlsx\n","No_energy.xlsx\n","No_veterinary.xlsx\n","No_Medicine.xlsx\n","No_Arts_and_humanities.xlsx\n"]}],"source":["# add subject area column and merge all excels in the folder\n","import os\n","\n","directory = os.fsencode(\"./no_excels\")\n","\n","excels_to_merge = []\n","    \n","for file in os.listdir(directory):\n","     filename = os.fsdecode(file)\n","     excel = pd.read_excel(\"./no_excels/\"+filename,sheet_name = 0)\n","     print(filename)\n","     excel[\"Subject Area\"] = [None]*excel.shape[0]\n","\n","     for i in range(excel.shape[0]):\n","          excel.iloc[i,6] = filename[3:-5]\n","     \n","     #excel.to_excel(\"./no_excels/\"+filename, sheet_name = \"1\", index = False)\n","     \n","     excels_to_merge.append(excel)\n","\n","main_excel = pd.concat(excels_to_merge)\n","main_excel.to_excel(\"./no_excels/\"+\"no_labels.xlsx\", sheet_name = \"1\", index = False)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def get_arxiv_metadata(file_loc):\n","    with open(file_loc) as f:\n","        for line in f:\n","            yield line\n","\n","def get_arxiv_AI_ML_papers(arxiv_file):\n","    import json\n","\n","    arxiv_metadata = get_arxiv_metadata('arxiv-metadata-oai-snapshot.json')\n","\n","    arxiv_AI_ML_abstracts = []\n","    arxiv_AI_ML_subject = []\n","    arxiv_AI_ML_years = []\n","    arxiv_AI_ML_journals = []\n","\n","    for paper in arxiv_metadata:\n","        paper = json.loads(paper)\n","        \n","        if ( paper['categories'].__contains__('cs.AI') or paper['categories'].__contains__('cs.LG') ):\n","            arxiv_AI_ML_abstracts.append(paper['abstract'])\n","\n","            if ( paper['categories'].__contains__('cs.AI') and paper['categories'].__contains__('cs.LG') ):\n","                arxiv_AI_ML_subject.append(\"cs.AI;cs.LG\")\n","            elif 'cs.AI' in paper['categories']:\n","                arxiv_AI_ML_subject.append(\"cs.AI\")\n","            else:\n","                arxiv_AI_ML_subject.append(\"cs.LG\")\n","            \n","            arxiv_AI_ML_journals.append(paper['journal-ref'])\n","\n","            for str_idx in range( len(paper['id'])-1 ):\n","                if (paper['id'][str_idx:str_idx+2].isnumeric()):\n","                    if int(paper['id'][str_idx:str_idx+2])<22:\n","                        arxiv_AI_ML_years.append('20'+paper['id'][str_idx:str_idx+2])\n","                    else:\n","                        arxiv_AI_ML_years.append('19'+paper['id'][str_idx:str_idx+2])\n","                    break\n","    \n","    return (np.array(arxiv_AI_ML_abstracts), np.array(arxiv_AI_ML_subject), np.array(arxiv_AI_ML_years), np.array(arxiv_AI_ML_journals))\n","\n","def encode_numpy_utf(input):\n","    size = np.shape(input)[0]\n","\n","    result = np.empty_like(input)\n","    result[:(size//2)] = np.char.encode(input[:size//2].astype(str), 'utf-8')\n","    result[(size//2):] = np.char.encode(input[size//2:].astype(str), 'utf-8')\n","\n","    return result\n","\n","\n","def get_excel_data(filename,columns_to_get,sheet=0):\n","    excel = pd.read_excel(filename,sheet)\n","    result_list = []\n","\n","    for column in columns_to_get:\n","        if column is None:\n","            result_list.append(None)\n","        else:\n","            temp = excel.loc[:,[column]]\n","            result_list.append(temp.to_numpy().squeeze())\n","    \n","    return result_list\n","\n","def find_nan(array):\n","    cnt=0\n","    for idx,stuff in enumerate(array):\n","        if (not isinstance(stuff,str)):\n","            print(stuff)\n","            print(idx)\n","            cnt+=1\n","            if(cnt>100):break\n","\n","def read_split_data(arxiv=True):\n","    yes_columns = ['Abstract',None,None,'Source title']\n","    no_columns = ['Abstract','Subject Area',None,'Source title']\n","    yes_list = get_excel_data('AI_Classification_Toy_Dataset_2.xlsx',yes_columns,'yes')\n","    no_list = get_excel_data('no_labels.xlsx',no_columns)\n","\n","    yes_list[1] = np.array(len(yes_list[0]) * ['AI'])\n","\n","    yes_list[2] = np.array(len(yes_list[0]) * ['2000']) #TEMP UNTIL I GET DEM YEARS\n","    no_list[2] = np.array(len(no_list[0]) * ['2000']) #TEMP UNTIL I GET DEM YEARS\n","\n","\n","    if(arxiv):\n","        arxiv_data = get_arxiv_AI_ML_papers('arxiv-metadata-oai-snapshot.json')\n","        \n","        for i in range(len(yes_list)):\n","            yes_list[i] = np.append(yes_list[i],arxiv_data[i])\n","\n","\n","    p = np.random.permutation(len(no_list[0]))\n","    for i in range(len(no_list)):\n","        no_list[i] = no_list[i][p]\n","    \n","    p = np.random.permutation(len(yes_list[0]))\n","    for i in range(len(yes_list)):\n","        yes_list[i] = yes_list[i][p]\n","\n","    \n","    test_list = [None] * len(yes_columns)\n","\n","    split_index_no = int( len(no_list[0])*0.5 )\n","    for i in range(len(no_list)):\n","        test_list[i] = no_list[i][split_index_no:]\n","        no_list[i] = no_list[i][:split_index_no]\n","\n","\n","    yes_list.append( np.ones(len(yes_list[0]),dtype=np.int32) )\n","    no_list.append( np.zeros(len(no_list[0]),dtype=np.int32) )\n","\n","        \n","\n","    train_list = [None] * len(yes_list)\n","    val_list = [None] * len(yes_list)\n","\n","    split_index_no = int( len(no_list[0])*0.75 )\n","    split_index_yes = int( len(yes_list[0])*0.75 )\n","    for i in range(len(yes_list)):\n","        train_list[i] = np.append(yes_list[i][:split_index_yes],no_list[i][:split_index_no])\n","        val_list[i] = np.append(yes_list[i][split_index_yes:],no_list[i][split_index_no:])\n","\n","    p = np.random.permutation(len(train_list[0]))\n","    for i in range(len(train_list)):\n","        train_list[i] = train_list[i][p]\n","    \n","    p = np.random.permutation(len(val_list[0]))\n","    for i in range(len(val_list)):\n","        val_list[i] = val_list[i][p]\n","\n","    return train_list,val_list,test_list\n","\n","\n","def create_dataset_file(arxiv=True):\n","\n","    train_list,val_list,test_list = read_split_data(arxiv)\n","\n","\n","    dataset_dict = {'train_set' : {'abstract':train_list[0].tolist(),'labels':train_list[4].tolist(),'Subject Area':train_list[1].tolist(),'Year':train_list[2].tolist(),'Journal':train_list[3].tolist()},\n","                    'val_set': {'abstract':val_list[0].tolist(),'labels':val_list[4].tolist(),'Subject Area':val_list[1].tolist(),'Year':val_list[2].tolist(),'Journal':val_list[3].tolist()},\n","                    'test_set': {'abstract':test_list[0].tolist(),'Subject Area':test_list[1].tolist(),'Year':test_list[2].tolist(),'Journal':test_list[3].tolist()} }\n","\n","    import json\n","    with open(\"AI_Paper_Classification_Dataset2.json\", 'w') as json_file:\n","        json.dump(dataset_dict, json_file)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["done\n"]}],"source":["create_dataset_file()\n","print(\"done\")\n","#arxiv_abstracts,arxiv_subjects,arxiv_years = get_arxiv_AI_ML_papers('arxiv-metadata-oai-snapshot.json')"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["316535\n","129371\n","105512\n","43124\n","249552\n"]}],"source":["print(len(train_set[0]))\n","print(np.count_nonzero( train_set[1] ))\n","\n","print(len(val_set[0]))\n","print(np.count_nonzero( val_set[1] ))\n","\n","print(len(test_set[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyPluOjuDu2gYWa0ozygev/9","collapsed_sections":[],"name":"AI_Classification.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}
